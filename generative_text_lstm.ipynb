{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Herein are cells that \n",
    "# 1. Create a text class that has methods to determine the vocabulary\n",
    "# within the text, encode/decode the text, split the text content into\n",
    "# validation, training and test sets.\n",
    "# \n",
    "# 2. LSTM language model for training and generating text.\n",
    "#\n",
    "# 3. Reading in of a simple test case\n",
    "#  Training and validation loop for simple test case.\n",
    "#\n",
    "# 4. Tiny Shakespeare training and generation. \n",
    "# \n",
    "\n",
    "# Refactoring - there is a logic in moving the vocab/encoding/decoding \n",
    "# methods from the text class into the language model, since the language\n",
    "# model has to deal with with an appropriately encoded/decoded \n",
    "# set of data - so saving the weights of the language model is useless\n",
    "# without knowing the encodings! On the otherhand ... that's to \n",
    "# put two different types of things in the same object ... so leave as\n",
    "# is for the moment. \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class text:\n",
    "    def __init__(self, text_name, is_file):\n",
    "        \n",
    "        # Retrieve text\n",
    "        self.text = ''\n",
    "        self.encoded = ''\n",
    "        if is_file == True:\n",
    "            print(\"reading text from file\")\n",
    "            self.from_file(text_name)\n",
    "        else:\n",
    "            self.from_string(text_name)\n",
    "        \n",
    "        # Calculate vocab size, i.e. the number of characters; \n",
    "        # first get sorted list of unique characters\n",
    "        self.chars = sorted(list(set(self.text)))\n",
    "        #self.vocab_size = self.vocab_size(chars)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.str_to_int = {}\n",
    "        self.int_to_str = {}\n",
    "        self.str_to_int = self.make_str_to_int_table(self.chars)\n",
    "        self.int_to_str = self.make_int_to_str_table(self.chars)\n",
    "    \n",
    "    def from_file(self, filename):\n",
    "        'Read text from file and calculate vocab size'\n",
    "        self.text = open(filename,'r',encoding='utf-8').read()\n",
    "    \n",
    "    def from_string(self, string):\n",
    "        'Read text from string and calculate vocab size'\n",
    "        self.text = string\n",
    "    \n",
    "    #def calc_vocab_size(self, chars):\n",
    "    #    'Calculate vocab size'\n",
    "    #    self.vocab_size = len(chars)\n",
    "        \n",
    "    def make_str_to_int_table(self, chars):\n",
    "        '''Populate dictionary of character to integer mapping'''\n",
    "        return  {character: int for int, character in enumerate(chars)}\n",
    "    \n",
    "    def make_int_to_str_table(self, chars):\n",
    "        '''Populate dictionary of character to integer mapping'''\n",
    "        return  {int: character for int, character in enumerate(chars)}\n",
    "    \n",
    "    def encode_text_as_tensor(self):\n",
    "        '''encode the training text as a list of integers \n",
    "        and then convert to tensor with which to replace self.text'''\n",
    "        encode = lambda char:[self.str_to_int[char] for char in self.text]\n",
    "        self.text = torch.tensor(encode(self.text), dtype = torch.long)\n",
    "    \n",
    "    def encode_new_text_as_tensor(self, to_encode):\n",
    "        '''encode a new text as a list of integers, according to the \n",
    "        encoding derived from the training text. Return a tensor'''\n",
    "        encode = lambda char:[self.str_to_int[char] for char in to_encode]\n",
    "        return torch.tensor(encode(to_encode), dtype = torch.long)\n",
    "\n",
    "    def decode(self, to_decode):\n",
    "        '''decode from a list of integers to a string, using the\n",
    "        encoding vocabulary attached to the object: self.int_to_str '''  \n",
    "        decode = lambda l: ''.join([self.int_to_str[i] for i in l])\n",
    "        return decode(to_decode)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.vocab_size}\"\n",
    "    \n",
    "    def make_train_val_test(self, fraction_train, fraction_val, fraction_test):\n",
    "        '''simple train /validation sets.  no randomisation \n",
    "         of selections, so assuming  no bias in the distribution within the data file'''\n",
    "        if fraction_train + fraction_test + fraction_val != 1:\n",
    "            print(\"Warning, fractions of train, test and validation do \\\n",
    "                  not add to one.\")\n",
    "        n = int(fraction_train*len(self.text))\n",
    "        nv = int(fraction_val*len(self.text))\n",
    "        nt = int(fraction_test*len(self.text))\n",
    "        self.train_data = self.text[:n]\n",
    "        self.val_data = self.text[n:n+nv]\n",
    "        self.test_data = self.text[n+nv:n+nv+nt]\n",
    "    \n",
    "    def get_batch(self, batch_size, block_size, train_test_validation):\n",
    "        \"\"\"Randomly pick data from the training data/test data/validation\n",
    "        and return as a batch stacked in a torch tensor.\"\"\"\n",
    "        if train_test_validation == \"train\":\n",
    "            data = self.train_data\n",
    "        elif train_test_validation == \"test\":\n",
    "            data = self.test_data\n",
    "        elif train_test_validation == \"validation\":\n",
    "            data = self.val_data\n",
    "        else:\n",
    "            raise \\\n",
    "            ValueError(\"Specify data set as 'train', 'test', or 'validation'.\")\n",
    "\n",
    "        if len(data) < block_size:\n",
    "            raise ValueError(\"Data is smaller than the specified block size.\")\n",
    "\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        train_context_batch = torch.stack([data[i:i + block_size] for i in ix])\n",
    "        train_to_predict_batch =\\\n",
    "              torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "        return train_context_batch, train_to_predict_batch\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(number_eval_batches, \n",
    "                  language_model, training_text_object, block_size):\n",
    "  '''Take number_eval_batches from training and validation sets and \n",
    "  calculate an average loss for each. Return a dictionary with the\n",
    "  two losses, with keys train and validation '''\n",
    "  out = {}\n",
    "  language_model.eval()\n",
    "  for split in ['train', 'validation']:\n",
    "    losses = torch.zeros(number_eval_batches)\n",
    "    for k in range(number_eval_batches):\n",
    "      X, Y = training_text_object.get_batch(1, block_size, split)\n",
    "      logits, loss = language_model(X, Y)\n",
    "      losses[k] = loss.item()\n",
    "    out[split] = losses.mean()\n",
    "  language_model.train()\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM  \n",
    "#\n",
    "class LanguageModel_LSTM(nn.Module):\n",
    "  '''logits produced from an LSTM over the context (length block_size) \n",
    "   of the previous characters, \n",
    "    from which we are trying to predict the current character; \n",
    "  generate() uses logits as a multivariate distribution from which \n",
    "    to predict next character. \n",
    "  \n",
    "   Training learns the values in the embedding vector, the weights of \n",
    "    RNN and the weights of the fully connected layer prior to the\n",
    "     softmax '''\n",
    "\n",
    "  def __init__(self, vocab_size, hidden_size, num_layers, verbose=False):\n",
    "    super().__init__()\n",
    "    self.verbose=verbose\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    # make all embeddings small so the logits produced give a\n",
    "    # probability distribution is equal in all direction.\n",
    "    # maximum initial entropy should give the most likely low cross entropy, \n",
    "    # since initial guess is not likely to be better than equal uncertainty.\n",
    "    with torch.no_grad():\n",
    "      self.token_embedding_table.weight.data \\\n",
    "      = self.token_embedding_table.weight.data * 0.01\n",
    "    self.input_size = vocab_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers,\\\n",
    "                       batch_first=True) # (h0, c0) default to zero.\n",
    "    self.fully_connected = nn.Sequential(\n",
    "                       nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(self.hidden_size, vocab_size)\n",
    "                       )\n",
    "\n",
    "    \n",
    "  def set_verbose(self, verbose):\n",
    "    self.verbose = verbose\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # idx - (index of) the x values, i.e. the context vector\n",
    "    # idx and targets are both (B, T) tensor of integers (see comment below)\n",
    "    #h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "    # default h0 for RNN model  is to set everything to zeros\n",
    "    logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "    # nn.RNN produces batch_size, sequ, hidden_size,\n",
    "    out, _ = self.lstm(logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #logits = self.fully_connected(logits)\n",
    "    logits = self.fully_connected(out)\n",
    "    # B T C is batch by time by channel\n",
    "    # batch  is number of the batch\n",
    "    # time is block size\n",
    "    # channel is the vocab size \n",
    "    if targets == None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape # need to reshape to be correct shape for cross_entropy\n",
    "      # use view in pytorch that changes the view of the data passed but not the\n",
    "      # underlying data. Flatten the first two dimensions of the logits, \n",
    "      # to create\n",
    "      # a batch of size B*T with the channel data,\n",
    "      # i.e. vocab or probability of each character/class as the second\n",
    "      # dimension, which is what F.cross_entropy expects.\n",
    "      # Similarly, targets reduced to 1 dimension of batch data (B*T) \n",
    "      # with class lable in each dimension. \n",
    "      if self.verbose: print(f\"Targets: \\n {targets} \\n; logits: \\n{logits} \")\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      if self.verbose: print(f\"Transformed view: Targets: \\n {targets} \\n; logits:\\n{logits}\\n\")\n",
    "      # cross_entropy includes a softmax transformation. \n",
    "      if self.verbose:\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        idx_next = torch.multinomial(probs, num_samples = 1) \n",
    "        print(f\"Generative outcomes during training:\\n {idx_next}\")\n",
    "\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    '''Predict the next character/token from the learnt distribution,adding\n",
    "    it to the current context, idx, till max_new_tokens have been added.'''\n",
    "    for _ in range(max_new_tokens):\n",
    "      #get the predictions\n",
    "      logits, _ = self(idx) # calls forwards() for this class\n",
    "      \n",
    "      # Following line: \n",
    "      # h_out from the last character of each sequence in the batch.\n",
    "      logits = logits[:, -1, :] \n",
    "      # logits now has dimension B, C, since only one token.\n",
    "      \n",
    "      # Generate probability distribution\n",
    "      probs = F.softmax(logits, dim=-1) # B, C\n",
    "      if self.verbose:\n",
    "          print(f'logits {logits}, {logits.shape} \\n probs {probs}, {probs.shape}')\n",
    "      \n",
    "      # Sample  vocabulary according to the probability distribution, probs.\n",
    "      idx_next = torch.multinomial(probs, num_samples = 1) # B by 1 size\n",
    "      # Append sample to sequence\n",
    "      idx = torch.cat((idx, idx_next), dim = 1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  ..., 25, 26,  0])\n",
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\customer\\AppData\\Local\\Temp\\ipykernel_2236\\194577180.py:73: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  self.text = torch.tensor(encode(self.text), dtype = torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Train language model on \"ABCD EFGH IJKL MNOP QRSTU VWXYZ \" string.\n",
    "# Define string\n",
    "# Read string into object\n",
    "# Encode string and place in a tensor\n",
    "string = \"ABCD EFGH IJKL MNOP QRSTU VWXYZ \"\n",
    "string = string * 100\n",
    "AB_string_3 = text(string, is_file=False)\n",
    "AB_string_3.encode_text_as_tensor()\n",
    "AB_string_3.make_train_val_test(0.8, 0.1, 0.1)\n",
    "print(AB_string_3.train_data)\n",
    "print(AB_string_3.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current iteration loss, at step 0: 3.2831077575683594\n",
      "step 0: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 3.3302,           \n",
      " validation loss 3.3374\n",
      "\n",
      "\n",
      "current iteration loss, at step 500: 0.893225908279419\n",
      "step 500: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 1.0081,           \n",
      " validation loss 1.1495\n",
      "\n",
      "\n",
      "current iteration loss, at step 1000: 0.36434099078178406\n",
      "step 1000: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.1959,           \n",
      " validation loss 0.2853\n",
      "\n",
      "\n",
      "current iteration loss, at step 1500: 0.036115728318691254\n",
      "step 1500: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.2230,           \n",
      " validation loss 0.0806\n",
      "\n",
      "\n",
      "current iteration loss, at step 2000: 0.24428200721740723\n",
      "step 2000: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.0996,           \n",
      " validation loss 0.0669\n",
      "\n",
      "\n",
      "current iteration loss, at step 2500: 0.010101783089339733\n",
      "step 2500: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.0861,           \n",
      " validation loss 0.1405\n",
      "\n",
      "\n",
      "current iteration loss, at step 3000: 0.0036862387787550688\n",
      "step 3000: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.2536,           \n",
      " validation loss 0.0591\n",
      "\n",
      "\n",
      "current iteration loss, at step 3500: 0.0032198650296777487\n",
      "step 3500: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.1548,           \n",
      " validation loss 0.0970\n",
      "\n",
      "\n",
      "final loss: \n",
      "         loss averaged over 10 batches:        \n",
      " training loss: 0.0452,         \n",
      " validation loss 0.0403\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size = AB_string_3.vocab_size\n",
    "input_size = AB_string_3.vocab_size\n",
    "num_layers = 1\n",
    "m3 = LanguageModel_LSTM(input_size, hidden_size, num_layers, verbose=False)\n",
    "\n",
    "# check code is doing what I expect it to do. \n",
    "# Keep batch_size and block_size small to allow easy visualisation of \n",
    "# what is going on inside the network, to check that it's doing what I \n",
    "# think it's doing.\n",
    "# Uncommented print statements in code\n",
    "batch_size=2\n",
    "block_size =4\n",
    "number_of_evaluation_batches = 10\n",
    "evaluation_interval = 500\n",
    "# high learning rate since simple network\n",
    "learning_rate = 1e-3\n",
    "#logits, loss = m(train_context_batch, train_to_predict_batch)\n",
    "#print(logits.shape) \n",
    "#print(loss)\n",
    "\n",
    "# training loop. \n",
    "optimizer = torch.optim.AdamW(m3.parameters(), lr=learning_rate) \n",
    "for step in range(4000):\n",
    "  # get a new batch\n",
    "  xb, yb = AB_string_3.get_batch(batch_size, block_size, 'train')\n",
    "  logits, loss = m3(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "   #equally uncertain distribution implies 1/vocab_size = 1/65 probability of each \n",
    "   # next character which implies -ln(1/65) = 4.17 would be optimal initial loss\n",
    "  if step % evaluation_interval == 0: \n",
    "    print(f\"current iteration loss, at step {step}: {loss.item()}\")\n",
    "  if step % evaluation_interval == 0:\n",
    "    #logits_val, loss_val = m(xval, yval)\n",
    "    #print(f\"training loss: {loss.item()}, \\\n",
    "    #                                       validation loss {loss_val.item()}\")\n",
    "    losses = estimate_loss(number_of_evaluation_batches, m3, AB_string_3, block_size)\n",
    "    print(f\"step {step}: \\n \\\n",
    "           loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "           \\n training loss: {losses['train']:.4f}, \\\n",
    "          \\n validation loss {losses['validation']:.4f}\\n\\n\")\n",
    "losses = estimate_loss(number_of_evaluation_batches, m3, AB_string_3, block_size)\n",
    "print(f\"final loss: \\n \\\n",
    "        loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "        \\n training loss: {losses['train']:.4f}, \\\n",
    "        \\n validation loss {losses['validation']:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([[0]])\n",
      "tensor([[ 0, 13, 14, 15, 16,  0, 17, 18, 19, 20, 21,  0, 22, 23, 24, 25, 26,  0,\n",
      "          1,  2,  3,  4,  0,  5,  6,  7]])\n",
      " MNOP QRSTU VWXYZ ABCD EFG\n"
     ]
    }
   ],
   "source": [
    "context = ' '\n",
    "context = AB_string_3.encode_new_text_as_tensor(context)\n",
    "print(context)\n",
    "context = torch.unsqueeze(context, dim=0)\n",
    "print(context)\n",
    "m3.set_verbose(False)\n",
    "new_string = m3.generate(context,25)\n",
    "print(new_string)\n",
    "print(AB_string_3.decode(new_string[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeare\n",
      "reading text from file\n",
      "chars, ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "vocab size: 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\customer\\AppData\\Local\\Temp\\ipykernel_2904\\194577180.py:73: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  self.text = torch.tensor(encode(self.text), dtype = torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Next, tiny Shakespeare\n",
    "print(\"shakespeare\")\n",
    "shakespeare = text(\"input.txt\", is_file=True)\n",
    "#shakespeare.from_file(\"input.txt\")\n",
    "print(f\"chars, {shakespeare.chars}\")\n",
    "print(f\"vocab size: {shakespeare.vocab_size}\")\n",
    "shakespeare.encode_text_as_tensor()\n",
    "shakespeare.make_train_val_test(0.8, 0.1, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current iteration loss, at step 0: 4.160330772399902\n",
      "step 0: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 4.1506,           \n",
      " validation loss 4.1510\n",
      "\n",
      "\n",
      "current iteration loss, at step 500: 2.5889129638671875\n",
      "step 500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 2.5823,           \n",
      " validation loss 2.6899\n",
      "\n",
      "\n",
      "current iteration loss, at step 1000: 2.3099141120910645\n",
      "step 1000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 2.3156,           \n",
      " validation loss 2.2898\n",
      "\n",
      "\n",
      "current iteration loss, at step 1500: 2.172563076019287\n",
      "step 1500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 2.1323,           \n",
      " validation loss 2.1592\n",
      "\n",
      "\n",
      "current iteration loss, at step 2000: 2.0868782997131348\n",
      "step 2000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.9817,           \n",
      " validation loss 2.0954\n",
      "\n",
      "\n",
      "current iteration loss, at step 2500: 1.9080678224563599\n",
      "step 2500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.9565,           \n",
      " validation loss 2.0402\n",
      "\n",
      "\n",
      "current iteration loss, at step 3000: 1.940613865852356\n",
      "step 3000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.8827,           \n",
      " validation loss 1.9535\n",
      "\n",
      "\n",
      "current iteration loss, at step 3500: 1.8557989597320557\n",
      "step 3500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.8596,           \n",
      " validation loss 1.8603\n",
      "\n",
      "\n",
      "final loss: \n",
      "         loss averaged over 20 batches:        \n",
      " training loss: 1.7140,         \n",
      " validation loss 1.9294\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size = shakespeare.vocab_size*2\n",
    "input_size = shakespeare.vocab_size\n",
    "num_layers = 3\n",
    "shakespeare_predictor = LanguageModel_LSTM(input_size, \\\n",
    "                                hidden_size, num_layers, verbose=False)\n",
    "\n",
    "# check code is doing what I expect it to do. \n",
    "# Keep batch_size and block_size small to allow easy visualisation of \n",
    "# what is going on inside the network, to check that it's doing what I \n",
    "# think it's doing.\n",
    "\n",
    "batch_size=32\n",
    "block_size =100\n",
    "number_of_evaluation_batches = 20\n",
    "evaluation_interval = 500\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# training loop. \n",
    "optimizer =\\\n",
    "      torch.optim.AdamW(shakespeare_predictor.parameters(), lr=learning_rate) \n",
    "for step in range(4000):\n",
    "  # get a new batch\n",
    "  xb, yb = shakespeare.get_batch(batch_size, block_size, 'train')\n",
    "  logits, loss = shakespeare_predictor(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  if step % evaluation_interval == 0: \n",
    "    print(f\"current iteration loss, at step {step}: {loss.item()}\")\n",
    "  if step % evaluation_interval == 0:\n",
    "    losses = estimate_loss(number_of_evaluation_batches, \\\n",
    "                           shakespeare_predictor, shakespeare, block_size)\n",
    "    print(f\"step {step}: \\n \\\n",
    "           loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "           \\n training loss: {losses['train']:.4f}, \\\n",
    "          \\n validation loss {losses['validation']:.4f}\\n\\n\")\n",
    "losses = estimate_loss(number_of_evaluation_batches,\\\n",
    "                        shakespeare_predictor, shakespeare, block_size)\n",
    "print(f\"final loss: \\n \\\n",
    "        loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "        \\n training loss: {losses['train']:.4f}, \\\n",
    "        \\n validation loss {losses['validation']:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current iteration loss, at step 0: 1.766512155532837\n",
      "step 0: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.9968,           \n",
      " validation loss 2.1066\n",
      "\n",
      "\n",
      "current iteration loss, at step 500: 1.767135500907898\n",
      "step 500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.6889,           \n",
      " validation loss 1.7798\n",
      "\n",
      "\n",
      "current iteration loss, at step 1000: 1.7006516456604004\n",
      "step 1000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.6787,           \n",
      " validation loss 1.7881\n",
      "\n",
      "\n",
      "current iteration loss, at step 1500: 1.6458576917648315\n",
      "step 1500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.6490,           \n",
      " validation loss 1.7993\n",
      "\n",
      "\n",
      "current iteration loss, at step 2000: 1.6739484071731567\n",
      "step 2000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.6966,           \n",
      " validation loss 1.7325\n",
      "\n",
      "\n",
      "current iteration loss, at step 2500: 1.5582492351531982\n",
      "step 2500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.6562,           \n",
      " validation loss 1.8380\n",
      "\n",
      "\n",
      "current iteration loss, at step 3000: 1.5618102550506592\n",
      "step 3000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.5886,           \n",
      " validation loss 1.7625\n",
      "\n",
      "\n",
      "current iteration loss, at step 3500: 1.5511994361877441\n",
      "step 3500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.6344,           \n",
      " validation loss 1.6995\n",
      "\n",
      "\n",
      "current iteration loss, at step 4000: 1.6088865995407104\n",
      "step 4000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.5252,           \n",
      " validation loss 1.6713\n",
      "\n",
      "\n",
      "current iteration loss, at step 4500: 1.5486187934875488\n",
      "step 4500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.6476,           \n",
      " validation loss 1.6546\n",
      "\n",
      "\n",
      "current iteration loss, at step 5000: 1.4792754650115967\n",
      "step 5000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.5552,           \n",
      " validation loss 1.6883\n",
      "\n",
      "\n",
      "current iteration loss, at step 5500: 1.5033668279647827\n",
      "step 5500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.5632,           \n",
      " validation loss 1.6827\n",
      "\n",
      "\n",
      "current iteration loss, at step 6000: 1.5436608791351318\n",
      "step 6000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4976,           \n",
      " validation loss 1.6959\n",
      "\n",
      "\n",
      "current iteration loss, at step 6500: 1.5211668014526367\n",
      "step 6500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4789,           \n",
      " validation loss 1.7037\n",
      "\n",
      "\n",
      "current iteration loss, at step 7000: 1.481056809425354\n",
      "step 7000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4099,           \n",
      " validation loss 1.5783\n",
      "\n",
      "\n",
      "current iteration loss, at step 7500: 1.4337633848190308\n",
      "step 7500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.5408,           \n",
      " validation loss 1.5435\n",
      "\n",
      "\n",
      "current iteration loss, at step 8000: 1.419992208480835\n",
      "step 8000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4939,           \n",
      " validation loss 1.7518\n",
      "\n",
      "\n",
      "current iteration loss, at step 8500: 1.456117868423462\n",
      "step 8500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4790,           \n",
      " validation loss 1.5879\n",
      "\n",
      "\n",
      "current iteration loss, at step 9000: 1.4375849962234497\n",
      "step 9000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4054,           \n",
      " validation loss 1.6651\n",
      "\n",
      "\n",
      "current iteration loss, at step 9500: 1.4281138181686401\n",
      "step 9500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4554,           \n",
      " validation loss 1.5568\n",
      "\n",
      "\n",
      "final loss: \n",
      "         loss averaged over 20 batches:        \n",
      " training loss: 1.4177,         \n",
      " validation loss 1.5706\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# continue with another 10 000 steps - loss is coming down slowly\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# training loop. \n",
    "optimizer =\\\n",
    "      torch.optim.AdamW(shakespeare_predictor.parameters(), lr=learning_rate) \n",
    "for step in range(10000):\n",
    "  # get a new batch\n",
    "  xb, yb = shakespeare.get_batch(batch_size, block_size, 'train')\n",
    "  logits, loss = shakespeare_predictor(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  if step % evaluation_interval == 0: \n",
    "    print(f\"current iteration loss, at step {step}: {loss.item()}\")\n",
    "  if step % evaluation_interval == 0:\n",
    "    losses = estimate_loss(number_of_evaluation_batches, \\\n",
    "                           shakespeare_predictor, shakespeare, block_size)\n",
    "    print(f\"step {step}: \\n \\\n",
    "           loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "           \\n training loss: {losses['train']:.4f}, \\\n",
    "          \\n validation loss {losses['validation']:.4f}\\n\\n\")\n",
    "losses = estimate_loss(number_of_evaluation_batches,\\\n",
    "                        shakespeare_predictor, shakespeare, block_size)\n",
    "print(f\"final loss: \\n \\\n",
    "        loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "        \\n training loss: {losses['train']:.4f}, \\\n",
    "        \\n validation loss {losses['validation']:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([[1]])\n",
      "tensor([[ 1, 57, 53, 50, 42, 47, 52, 45,  6,  0, 21, 57,  1, 57, 53,  1, 46, 43,\n",
      "         56, 43,  1, 47, 57,  1, 57, 53,  1, 42, 39, 59, 45, 46, 58, 43, 56,  7,\n",
      "         42, 39, 63, 43, 42,  1, 61, 47, 58, 46,  1, 63, 53, 59,  0, 21, 52,  1,\n",
      "         17, 52, 45, 50, 39, 52, 42, 47, 52, 45,  1, 61, 39, 56, 57,  1, 53, 44,\n",
      "          1, 51, 63,  1, 53, 47, 52,  1, 47, 52, 44, 47, 56, 51,  8,  0,  0, 30,\n",
      "         21, 15, 20, 13, 30, 16, 10,  0, 25, 63,  1]])\n",
      " solding,\n",
      "Is so here is so daughter-dayed with you\n",
      "In Englanding wars of my oin infirm.\n",
      "\n",
      "RICHARD:\n",
      "My \n"
     ]
    }
   ],
   "source": [
    "# current model seems to be decreasing in loss, albeit very slowly\n",
    "# try some generative function and then train some more.\n",
    "context = ' '\n",
    "context = shakespeare.encode_new_text_as_tensor(context)\n",
    "print(context)\n",
    "context = torch.unsqueeze(context, dim=0)\n",
    "print(context)\n",
    "shakespeare_predictor.set_verbose(False)\n",
    "new_string = shakespeare_predictor.generate(context,100)\n",
    "print(new_string)\n",
    "print(shakespeare.decode(new_string[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current iteration loss, at step 0: 1.3689426183700562\n",
      "step 0: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.5214,           \n",
      " validation loss 1.7030\n",
      "\n",
      "\n",
      "current iteration loss, at step 500: 1.3954135179519653\n",
      "step 500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4450,           \n",
      " validation loss 1.5425\n",
      "\n",
      "\n",
      "current iteration loss, at step 1000: 1.396178126335144\n",
      "step 1000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.3731,           \n",
      " validation loss 1.5718\n",
      "\n",
      "\n",
      "current iteration loss, at step 1500: 1.421913743019104\n",
      "step 1500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4122,           \n",
      " validation loss 1.5844\n",
      "\n",
      "\n",
      "current iteration loss, at step 2000: 1.4302153587341309\n",
      "step 2000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.3774,           \n",
      " validation loss 1.6352\n",
      "\n",
      "\n",
      "current iteration loss, at step 2500: 1.3983981609344482\n",
      "step 2500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4761,           \n",
      " validation loss 1.7293\n",
      "\n",
      "\n",
      "current iteration loss, at step 3000: 1.4123623371124268\n",
      "step 3000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4105,           \n",
      " validation loss 1.4695\n",
      "\n",
      "\n",
      "current iteration loss, at step 3500: 1.3704606294631958\n",
      "step 3500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.3726,           \n",
      " validation loss 1.6040\n",
      "\n",
      "\n",
      "current iteration loss, at step 4000: 1.372861385345459\n",
      "step 4000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.3805,           \n",
      " validation loss 1.6181\n",
      "\n",
      "\n",
      "current iteration loss, at step 4500: 1.3984163999557495\n",
      "step 4500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.3804,           \n",
      " validation loss 1.4783\n",
      "\n",
      "\n",
      "final loss: \n",
      "         loss averaged over 20 batches:        \n",
      " training loss: 1.2961,         \n",
      " validation loss 1.5026\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# continue with another 10 000 steps - loss is coming down slowly\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# training loop. \n",
    "optimizer =\\\n",
    "      torch.optim.AdamW(shakespeare_predictor.parameters(), lr=learning_rate) \n",
    "for step in range(5000):\n",
    "  # get a new batch\n",
    "  xb, yb = shakespeare.get_batch(batch_size, block_size, 'train')\n",
    "  logits, loss = shakespeare_predictor(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  if step % evaluation_interval == 0: \n",
    "    print(f\"current iteration loss, at step {step}: {loss.item()}\")\n",
    "  if step % evaluation_interval == 0:\n",
    "    losses = estimate_loss(number_of_evaluation_batches, \\\n",
    "                           shakespeare_predictor, shakespeare, block_size)\n",
    "    print(f\"step {step}: \\n \\\n",
    "           loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "           \\n training loss: {losses['train']:.4f}, \\\n",
    "          \\n validation loss {losses['validation']:.4f}\\n\\n\")\n",
    "losses = estimate_loss(number_of_evaluation_batches,\\\n",
    "                        shakespeare_predictor, shakespeare, block_size)\n",
    "print(f\"final loss: \\n \\\n",
    "        loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "        \\n training loss: {losses['train']:.4f}, \\\n",
    "        \\n validation loss {losses['validation']:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([[1]])\n",
      "tensor([[ 1, 57, 54, 43, 39, 49,  1, 39, 50, 50, 53, 61,  8,  0,  0, 30, 13, 32,\n",
      "         15, 24, 13, 33, 31, 10,  0, 21, 44,  1, 58, 46, 43, 52,  1, 46, 43,  1,\n",
      "         41, 39, 52, 52, 53, 58,  1, 45, 43, 52, 58, 50, 43, 51, 64, 53, 56, 42,\n",
      "         10,  0, 18, 53, 56,  1, 58, 46, 59, 57,  1, 44, 53, 59, 52, 42,  1, 46,\n",
      "         47, 57,  1, 46, 39, 52, 42,  1, 53, 44,  1, 57, 59, 41, 49, 43, 57,  1,\n",
      "         63, 53, 59, 56,  1, 61, 39, 63, 57,  5, 58,  0, 35, 47, 58, 46,  1, 58,\n",
      "         46, 43,  1, 53, 56, 54, 50, 43, 57, 57, 47, 53, 52,  1, 57, 53, 52, 42,\n",
      "          1, 44, 53, 56,  1, 53, 59, 56,  1, 57, 47, 45, 46,  8,  0, 21, 44,  1,\n",
      "         21,  1, 42, 47, 42,  1, 52]])\n",
      " speak allow.\n",
      "\n",
      "RATCLAUS:\n",
      "If then he cannot gentlemzord:\n",
      "For thus found his hand of suckes your ways't\n",
      "With the orplession sond for our sigh.\n",
      "If I did n\n"
     ]
    }
   ],
   "source": [
    "# current model seems to be still decreasing in loss, albeit very slowly\n",
    "context = ' '\n",
    "context = shakespeare.encode_new_text_as_tensor(context)\n",
    "print(context)\n",
    "context = torch.unsqueeze(context, dim=0)\n",
    "print(context)\n",
    "shakespeare_predictor.set_verbose(False)\n",
    "new_string = shakespeare_predictor.generate(context,150)\n",
    "print(new_string)\n",
    "print(shakespeare.decode(new_string[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('token_embedding_table.weight',\n",
       "              tensor([[-0.3519,  0.8586,  0.2595,  ..., -0.2710,  0.5109,  0.2001],\n",
       "                      [-0.0978,  0.1085,  0.2313,  ..., -0.4648,  0.3872,  0.2498],\n",
       "                      [-0.3034,  0.6094,  0.0309,  ...,  0.1928,  0.4655,  0.5296],\n",
       "                      ...,\n",
       "                      [ 0.2113, -0.2238, -0.2096,  ...,  0.0266, -0.2321, -0.0184],\n",
       "                      [ 0.7435, -0.1425,  0.1972,  ..., -0.1706, -0.1924, -0.4187],\n",
       "                      [ 0.2101, -1.1720,  0.0276,  ..., -0.0334, -0.3433,  0.5295]])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[ 1.2527, -0.7727,  0.0292,  ..., -0.2207,  0.1949,  0.1861],\n",
       "                      [ 0.7808, -0.6094, -0.2140,  ...,  0.2757, -0.4334, -0.4543],\n",
       "                      [ 0.2404,  0.2741, -0.3262,  ...,  0.7664,  0.0780,  0.1539],\n",
       "                      ...,\n",
       "                      [-0.4716,  0.1124,  0.2938,  ..., -0.0517, -0.2308,  0.3878],\n",
       "                      [ 0.1763, -0.1046, -0.6929,  ...,  0.1044, -0.4679, -0.1496],\n",
       "                      [ 0.0957, -0.2789,  0.6450,  ..., -0.3583,  0.4004,  0.2568]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[-1.1683,  0.1583,  0.1219,  ...,  0.2079, -0.0445, -0.0508],\n",
       "                      [ 0.2702, -0.0104,  0.1443,  ..., -0.4637, -0.0813, -0.2232],\n",
       "                      [-0.3542,  0.0933, -0.1781,  ...,  0.0834,  0.0091, -0.2002],\n",
       "                      ...,\n",
       "                      [-0.1548, -0.2215, -0.0273,  ..., -0.0297, -0.0037, -0.1566],\n",
       "                      [-0.2004,  0.0527, -0.0410,  ..., -0.1103, -0.0790,  0.1115],\n",
       "                      [-0.0342, -0.1094, -0.5216,  ...,  0.0628, -0.0402, -0.1270]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([ 1.1033e-01,  1.4286e-01,  4.3732e-01,  3.9507e-01,  2.1516e-01,\n",
       "                       4.9947e-01,  4.6792e-01,  4.3509e-01, -1.5113e-01,  1.3195e-01,\n",
       "                      -5.5201e-02,  4.2983e-01,  6.3551e-01,  9.9095e-01,  5.8716e-01,\n",
       "                       4.1432e-01,  3.4775e-01, -6.2286e-02,  3.3500e-01,  1.1365e+00,\n",
       "                       1.0889e+00,  1.2489e-01,  8.0546e-01,  3.8805e-01,  5.6380e-01,\n",
       "                       3.0222e-02,  1.5374e-01,  1.6022e-01,  1.2160e-01,  6.4475e-01,\n",
       "                       7.9293e-02,  8.6695e-01,  6.9602e-02,  6.9696e-01,  4.6415e-01,\n",
       "                       5.8104e-01,  9.1529e-01,  7.0716e-01,  1.3242e-01,  8.5640e-02,\n",
       "                      -7.7427e-02,  1.8179e-01, -6.5340e-03,  4.6972e-01,  3.6276e-01,\n",
       "                       4.4950e-01,  5.1850e-01,  1.6019e-01,  1.3421e-01,  3.9314e-01,\n",
       "                       5.9019e-01,  4.6175e-01,  3.6848e-01,  2.0315e-01,  4.8579e-01,\n",
       "                       3.6434e-01,  2.4406e-02,  9.7397e-02, -1.8932e-02,  4.2874e-01,\n",
       "                       9.2008e-02,  4.4958e-01, -4.8597e-02,  1.5411e-02,  2.6885e-01,\n",
       "                       5.9680e-02,  9.0074e-01,  1.9689e-01,  1.3348e-01,  6.7224e-01,\n",
       "                       3.0732e-01,  1.1172e-02,  1.7582e-02,  4.7946e-01,  1.1433e+00,\n",
       "                       1.8737e-01,  3.5697e-01,  3.4127e-01, -3.4238e-02,  2.1246e-01,\n",
       "                       5.5359e-01,  9.2260e-02,  2.5731e-01,  2.0456e-01,  1.4946e-01,\n",
       "                      -6.8444e-02,  6.7474e-01,  2.1610e-01,  2.1399e-01, -3.1588e-02,\n",
       "                       5.4963e-01,  3.5861e-01,  7.1176e-02,  1.3364e-01,  6.5005e-01,\n",
       "                       4.4646e-01,  1.2550e-01, -1.4760e-02,  1.3626e-01,  4.6992e-01,\n",
       "                       4.0352e-01,  2.2038e-01,  8.2273e-01, -5.0070e-02,  5.9339e-01,\n",
       "                       1.0434e-01,  4.5333e-01,  1.4350e-01,  8.3910e-02,  7.1338e-01,\n",
       "                       1.6198e-01,  1.6357e-01,  2.6924e-01,  5.1542e-01,  1.0544e+00,\n",
       "                       1.9575e-01,  8.2842e-02, -9.8452e-02,  3.3517e-01,  3.5552e-01,\n",
       "                       3.0441e-01, -1.7971e-01,  4.1182e-02,  8.4407e-02,  5.2292e-01,\n",
       "                       5.3536e-02,  7.5163e-01,  2.9554e-01,  3.0303e-01,  2.1031e-01,\n",
       "                      -8.3607e-04,  8.3869e-02,  9.3984e-02, -1.1057e-02,  5.6924e-02,\n",
       "                       1.3322e-01, -1.0822e-02,  7.7047e-02, -1.2621e-01,  1.7332e-02,\n",
       "                      -1.0719e-01, -4.1167e-02,  4.2341e-02,  4.0068e-02,  1.0528e-02,\n",
       "                       8.8417e-02, -5.5348e-02,  5.2393e-02, -1.0935e-01, -7.8413e-02,\n",
       "                       7.5550e-02, -1.5457e-01,  4.2759e-02,  7.9748e-02,  1.2753e-01,\n",
       "                      -4.9420e-02, -3.1894e-02,  6.0086e-02,  1.0004e-01, -6.7646e-03,\n",
       "                       1.4377e-02,  1.7760e-02, -3.8766e-03, -1.5283e-02,  8.0971e-02,\n",
       "                       6.5618e-02,  7.9613e-02,  7.8239e-02, -1.6551e-02, -1.3942e-02,\n",
       "                      -6.7101e-02, -1.0087e-01, -1.3742e-01,  1.2557e-01,  2.5937e-02,\n",
       "                       7.9687e-02,  7.0168e-02, -4.4227e-02,  3.1243e-03, -6.9510e-03,\n",
       "                       8.6457e-02,  1.1760e-01,  1.2761e-01,  3.1800e-02,  6.9827e-02,\n",
       "                       2.1334e-01, -1.3644e-02,  4.8979e-02, -6.1818e-02,  3.8644e-02,\n",
       "                      -8.4625e-02, -1.1893e-01, -5.0845e-02, -3.8293e-02,  1.7665e-01,\n",
       "                      -5.3436e-02,  1.1017e-01,  7.5917e-02,  1.0537e-01, -4.9573e-02,\n",
       "                      -1.0333e-02, -3.5055e-02,  1.5737e-02,  6.4863e-02,  1.4148e-01,\n",
       "                       5.6239e-02,  2.5732e-02, -1.3197e-01, -4.4160e-02,  1.5725e-02,\n",
       "                       6.0850e-02, -5.0380e-02, -6.0490e-02, -1.1385e-01,  3.6793e-03,\n",
       "                      -8.0780e-02,  1.6948e-01,  1.7744e-01, -8.8605e-02, -5.0514e-02,\n",
       "                       1.8222e-02,  6.1679e-02, -1.7479e-02, -5.6036e-02,  7.1190e-02,\n",
       "                      -9.8376e-03, -1.5457e-01,  1.7101e-01, -6.4603e-02,  1.5116e-01,\n",
       "                       5.7523e-02,  7.4856e-02, -1.2357e-02, -6.6082e-02,  6.9880e-03,\n",
       "                       3.0392e-02,  2.8609e-02, -6.1366e-03,  3.2780e-02,  2.3106e-02,\n",
       "                       1.2594e-02,  6.5094e-02,  1.5109e-01, -3.1049e-02,  6.4677e-03,\n",
       "                       2.6666e-02,  3.2839e-02, -2.1778e-02, -9.7937e-02,  5.6942e-02,\n",
       "                       7.3089e-02, -7.9097e-02, -1.0260e-01, -3.2892e-02,  8.2092e-02,\n",
       "                      -1.2935e-01, -6.5825e-02,  1.8525e-02, -1.9592e-03,  5.2868e-02,\n",
       "                      -1.4029e-01, -2.2555e-01, -4.9242e-01, -4.0628e-01, -1.1740e-01,\n",
       "                       5.8172e-01, -1.8956e-01, -3.6157e-01, -1.1850e-02, -1.0903e-02,\n",
       "                       4.9077e-02,  3.4792e-01, -5.4483e-01, -6.0644e-01,  5.5897e-01,\n",
       "                       2.1294e-01,  1.0556e-01, -5.4436e-02,  7.3139e-02, -5.9055e-01,\n",
       "                       5.6827e-01,  4.1281e-02, -5.1956e-01,  2.8046e-01, -4.6608e-01,\n",
       "                      -1.3333e-01,  9.9704e-02,  4.1325e-02, -1.9498e-01, -4.5896e-01,\n",
       "                       1.3390e-01, -5.2449e-01,  1.0240e-01, -3.4359e-01,  4.6998e-01,\n",
       "                      -3.6643e-01,  5.2915e-01,  5.5777e-01,  1.6201e-01,  3.1481e-02,\n",
       "                      -1.0208e-01,  1.3759e-02, -2.3779e-02, -3.6102e-01,  2.4544e-01,\n",
       "                      -2.4669e-01, -3.6496e-01, -1.3414e-01, -2.2086e-01, -3.3244e-01,\n",
       "                       4.1157e-01,  3.5625e-01,  3.1999e-01,  3.2075e-02,  1.9609e-01,\n",
       "                       1.1767e-01,  2.0029e-01, -1.9868e-01,  1.2395e-01,  2.3034e-01,\n",
       "                       2.0242e-01, -3.7223e-01, -1.1787e-01, -7.1962e-02, -9.4687e-02,\n",
       "                      -2.2447e-02,  3.8362e-01, -3.0460e-01,  2.7291e-01, -2.9794e-01,\n",
       "                       1.5321e-01,  1.1541e-02, -1.2473e-01,  3.6610e-01,  1.0284e+00,\n",
       "                       2.4132e-01, -3.6936e-02, -2.2757e-01, -2.6534e-01, -2.9800e-01,\n",
       "                       4.6305e-01,  7.5004e-02,  7.1547e-02, -1.1970e-01, -2.0648e-02,\n",
       "                      -1.7240e-01, -5.2813e-01, -2.1387e-01,  2.3984e-01,  8.1605e-02,\n",
       "                      -4.8117e-01,  3.2601e-01,  3.9847e-02,  5.7569e-03, -5.7318e-01,\n",
       "                       3.4327e-01,  8.0192e-02, -1.0438e-01,  8.5889e-02, -4.8494e-01,\n",
       "                       4.7022e-01,  1.9858e-01, -4.5383e-01,  8.0651e-02, -3.4669e-01,\n",
       "                       1.3617e-01, -1.9382e-01, -1.2847e-01,  5.0558e-02, -3.0965e-01,\n",
       "                      -1.4516e-01, -1.3241e-01,  1.8400e-01, -3.2740e-01, -8.6006e-01,\n",
       "                      -3.8580e-01,  1.9861e-01, -8.1291e-02,  7.3761e-02, -7.9111e-02,\n",
       "                       1.2231e-01,  4.7474e-02,  5.3509e-02, -7.9209e-02,  1.1727e-01,\n",
       "                      -2.5287e-02, -4.5744e-01,  4.6801e-01, -4.2213e-01, -3.1620e-01,\n",
       "                       3.4936e-01,  9.5131e-02,  1.7174e-01,  1.0329e-01,  3.3142e-02,\n",
       "                      -1.8681e-02,  4.2934e-01,  5.5647e-02,  5.9986e-01,  1.1997e-02,\n",
       "                      -1.1441e-01, -8.5669e-02,  6.3111e-02,  3.4491e-01,  1.3044e-01,\n",
       "                       2.3105e-01,  3.5530e-01,  7.2310e-02,  4.2409e-01,  3.0020e-01,\n",
       "                       2.8215e-01,  8.6438e-02,  1.5724e-01, -7.1095e-02,  1.2953e-01,\n",
       "                      -2.3774e-02,  1.6300e-01,  5.5761e-01,  1.4485e-01,  1.3129e-01,\n",
       "                       1.2791e-01,  1.6812e-01,  1.4042e-01,  1.6397e-01,  1.4775e-01,\n",
       "                       3.1003e-02,  1.8056e-01,  3.3454e-01,  9.6531e-02,  3.2525e-02,\n",
       "                       1.0891e-01,  2.8450e-01,  4.9693e-02,  2.0367e-01,  2.6570e-02,\n",
       "                       1.6210e-01,  1.9459e-01, -5.0112e-02,  9.6000e-02,  1.6272e-01,\n",
       "                       2.4668e-01,  5.2765e-02,  8.1812e-03, -4.1791e-03,  5.5830e-01,\n",
       "                       1.2005e-01,  2.9905e-02, -4.4594e-02, -1.2173e-02, -4.2308e-03,\n",
       "                       8.7634e-02, -1.3535e-01, -7.7891e-02,  1.4106e-01,  1.0577e-01,\n",
       "                       9.2019e-02,  2.4442e-01,  6.8898e-02,  3.1740e-01, -1.2253e-03,\n",
       "                       3.7773e-01,  1.3341e-01,  1.8230e-02,  1.3877e-01,  2.4100e-01,\n",
       "                       6.8335e-02,  1.4485e-01,  5.4711e-03,  5.8207e-01,  1.4675e-01,\n",
       "                       5.5975e-02,  4.5388e-01,  9.7544e-02,  1.2020e-01,  1.9575e-01,\n",
       "                      -8.0095e-02,  3.4456e-01,  7.9816e-03,  2.6367e-01,  2.1810e-01,\n",
       "                       2.6294e-02,  8.4376e-02,  2.6370e-02,  6.7510e-01,  1.2463e-01,\n",
       "                       6.7457e-02,  7.1943e-02,  1.4023e-01,  9.2310e-02,  2.7886e-02,\n",
       "                       1.5323e-01,  2.3699e-01,  2.6947e-01, -4.3032e-02,  4.0958e-02,\n",
       "                       2.4545e-01,  1.1835e-01, -2.7261e-02, -4.4084e-02,  2.7386e-01,\n",
       "                       3.1978e-01,  6.9923e-02,  2.0670e-02,  2.2829e-01,  1.8745e-01,\n",
       "                      -2.3935e-02,  8.2116e-02, -8.6450e-02,  4.3795e-01,  2.3341e-01,\n",
       "                       3.3954e-01,  7.7104e-02,  2.1301e-01,  2.4411e-01,  2.6902e-01,\n",
       "                       2.0001e-01,  2.6168e-01,  1.9324e-01,  1.8352e-01,  1.4092e-01])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 1.4753e-01,  8.0816e-02,  4.0419e-01,  3.5158e-01,  2.1234e-01,\n",
       "                       4.9999e-01,  4.9118e-01,  4.4682e-01, -8.8950e-02,  7.5833e-02,\n",
       "                      -5.4726e-02,  4.6756e-01,  6.7102e-01,  9.8286e-01,  6.7458e-01,\n",
       "                       4.0723e-01,  3.2300e-01, -7.5141e-02,  3.1853e-01,  1.1187e+00,\n",
       "                       1.0839e+00,  1.3159e-02,  8.2289e-01,  3.1649e-01,  5.6687e-01,\n",
       "                       7.9236e-02,  1.6136e-01,  2.7573e-01,  1.0714e-01,  6.8489e-01,\n",
       "                       3.3523e-02,  8.5645e-01,  1.1788e-01,  6.3122e-01,  4.8287e-01,\n",
       "                       5.8529e-01,  1.0106e+00,  7.9002e-01, -1.1442e-05,  5.2875e-02,\n",
       "                      -4.9164e-02,  1.4850e-01,  1.5012e-02,  5.3675e-01,  3.3221e-01,\n",
       "                       4.8880e-01,  5.1515e-01,  1.2066e-01,  5.7381e-02,  2.7146e-01,\n",
       "                       6.8202e-01,  4.4223e-01,  4.1620e-01,  2.2029e-01,  5.1658e-01,\n",
       "                       2.3290e-01,  7.0241e-02,  4.6876e-02, -2.0047e-02,  3.6376e-01,\n",
       "                       1.7746e-01,  4.1708e-01,  3.7140e-02,  8.1337e-02,  2.5166e-01,\n",
       "                      -5.5068e-03,  9.1300e-01,  1.7677e-01,  1.9473e-01,  6.5696e-01,\n",
       "                       2.9924e-01,  2.3859e-02,  5.5009e-02,  4.4496e-01,  1.2381e+00,\n",
       "                       2.0679e-01,  2.8713e-01,  3.0414e-01, -1.8416e-02,  3.0617e-01,\n",
       "                       5.6804e-01,  1.4635e-02,  2.0609e-01,  1.7660e-01,  1.3015e-01,\n",
       "                      -4.0285e-02,  6.7954e-01,  1.8858e-01,  2.0100e-01, -8.1299e-02,\n",
       "                       5.4615e-01,  3.5991e-01,  3.6567e-02,  2.5637e-01,  6.8941e-01,\n",
       "                       4.3368e-01,  8.4337e-02,  3.2037e-02,  1.3991e-02,  5.4167e-01,\n",
       "                       4.7612e-01,  3.3914e-01,  8.3511e-01, -7.1904e-02,  5.5291e-01,\n",
       "                      -1.3471e-02,  3.5506e-01,  1.7786e-01,  3.8869e-02,  6.4012e-01,\n",
       "                       1.9869e-01,  1.0401e-01,  2.8921e-01,  5.0199e-01,  1.0570e+00,\n",
       "                       1.9664e-01,  1.8618e-01, -1.5919e-01,  3.3025e-01,  2.6697e-01,\n",
       "                       2.4231e-01, -2.7798e-01,  3.1495e-02,  5.3612e-02,  4.6956e-01,\n",
       "                      -3.2193e-02,  7.2790e-01,  2.1613e-01,  3.1106e-01,  8.2344e-02,\n",
       "                       1.4040e-02,  1.9133e-02,  1.5011e-01,  5.6073e-02,  1.5882e-02,\n",
       "                       1.0835e-01, -8.3023e-02,  7.6328e-02, -1.4558e-01, -8.8168e-02,\n",
       "                      -2.2030e-03,  3.6489e-03,  1.8987e-02,  4.6418e-02,  4.5795e-02,\n",
       "                       1.2481e-01,  5.9876e-02,  1.5066e-02, -1.1329e-01, -5.4448e-02,\n",
       "                       8.0188e-03, -2.6075e-02,  3.6509e-02,  4.2736e-02,  1.2362e-01,\n",
       "                      -1.0629e-01,  6.8296e-03, -2.3583e-02,  1.0565e-01,  7.5199e-02,\n",
       "                      -5.8725e-03, -6.3405e-02,  5.5223e-02, -1.0578e-01,  5.7696e-02,\n",
       "                       3.7234e-02,  1.4656e-01,  6.6672e-02,  2.0621e-02, -5.3030e-02,\n",
       "                      -1.3003e-01, -1.0308e-01, -2.4757e-02,  1.5010e-01,  3.0162e-02,\n",
       "                       2.1139e-02, -1.5014e-02,  2.8699e-02, -5.8332e-02, -1.6570e-02,\n",
       "                       2.8354e-03,  1.8773e-01,  8.0994e-02,  5.6086e-02, -3.8684e-02,\n",
       "                       1.9946e-01,  2.6252e-02,  2.7973e-02,  3.9480e-03, -2.3526e-02,\n",
       "                      -7.6096e-02, -1.3124e-01, -2.3862e-03, -3.3977e-02,  1.1615e-01,\n",
       "                      -1.0512e-01,  1.7767e-01,  9.8283e-02,  1.1604e-01, -3.0290e-02,\n",
       "                      -3.2303e-02, -2.5306e-02,  8.4309e-02,  1.7531e-01,  4.8629e-02,\n",
       "                      -1.3656e-02, -5.5805e-02, -4.8270e-02,  6.6287e-02,  7.5339e-02,\n",
       "                       9.3615e-02,  6.3627e-03, -1.1183e-01, -1.4235e-01,  3.1457e-02,\n",
       "                      -4.1494e-02,  9.2418e-02,  1.5212e-01, -6.8235e-02, -7.7723e-02,\n",
       "                      -7.8088e-02, -5.3586e-03, -5.2002e-02, -1.4726e-02,  9.1773e-02,\n",
       "                       2.5046e-02, -2.5900e-01,  1.0504e-01, -3.6050e-02,  8.3416e-02,\n",
       "                       6.6558e-02,  1.0858e-01, -5.9873e-03, -6.1629e-02, -4.2790e-02,\n",
       "                       8.7960e-03, -3.9080e-02,  4.8086e-02, -1.9849e-02,  1.5813e-02,\n",
       "                       4.3732e-02,  5.9084e-02,  1.5063e-01,  4.5055e-03,  5.0941e-02,\n",
       "                       6.5710e-02,  5.9720e-02, -3.8698e-02, -1.0921e-01,  1.1137e-01,\n",
       "                      -2.8582e-02, -6.9999e-02, -1.5152e-01, -8.7802e-02, -4.7670e-03,\n",
       "                      -2.6791e-02, -7.0686e-02, -2.2912e-02,  1.1273e-01,  5.0367e-02,\n",
       "                      -5.1975e-02, -2.9184e-01, -4.1192e-01, -4.2927e-01, -8.1480e-02,\n",
       "                       6.2613e-01, -2.4644e-01, -3.2135e-01,  3.4029e-03,  1.5856e-04,\n",
       "                       2.2614e-02,  3.4311e-01, -4.8246e-01, -6.1567e-01,  5.1584e-01,\n",
       "                       2.2751e-01,  1.8280e-01, -1.0148e-01,  8.2894e-02, -5.5872e-01,\n",
       "                       5.7079e-01,  1.3192e-01, -4.8885e-01,  3.5003e-01, -4.8693e-01,\n",
       "                      -7.0115e-02,  1.2125e-01,  6.1434e-02, -1.4419e-01, -4.9231e-01,\n",
       "                       1.3003e-01, -5.1044e-01,  1.5337e-01, -2.9585e-01,  3.9567e-01,\n",
       "                      -2.9736e-01,  5.7027e-01,  4.8539e-01,  1.5851e-01,  9.0206e-02,\n",
       "                      -1.1789e-01,  5.4885e-02, -8.7152e-02, -4.1976e-01,  1.5041e-01,\n",
       "                      -2.5515e-01, -2.8517e-01, -1.8722e-01, -2.3314e-01, -3.1606e-01,\n",
       "                       4.0950e-01,  3.1526e-01,  3.0405e-01, -6.1600e-02,  1.5740e-01,\n",
       "                       3.8504e-02,  1.1001e-01, -2.1952e-01,  1.8274e-01,  1.9970e-01,\n",
       "                       1.9859e-01, -3.5919e-01, -1.1298e-02, -6.7511e-02, -5.1488e-02,\n",
       "                       7.7977e-02,  4.7500e-01, -3.3932e-01,  2.2617e-01, -3.6798e-01,\n",
       "                       1.6147e-01, -3.1891e-02, -7.7349e-02,  3.6123e-01,  9.4378e-01,\n",
       "                       1.4087e-01, -5.4524e-02, -3.4004e-01, -2.4809e-01, -2.5334e-01,\n",
       "                       4.1630e-01,  6.1686e-03, -3.3874e-03, -8.1588e-02,  2.4458e-02,\n",
       "                      -1.3664e-01, -4.7701e-01, -1.6183e-01,  2.7381e-01,  9.0133e-02,\n",
       "                      -4.7302e-01,  2.6368e-01,  1.4518e-01, -3.5667e-02, -5.5000e-01,\n",
       "                       2.5211e-01,  8.0215e-02, -8.8626e-02,  1.8044e-01, -4.7607e-01,\n",
       "                       3.9535e-01,  2.6085e-01, -4.4256e-01,  1.0827e-02, -3.3846e-01,\n",
       "                       1.2647e-01, -2.3679e-01, -2.1858e-01, -1.2138e-02, -2.3806e-01,\n",
       "                      -1.8064e-01, -9.5448e-02,  1.3182e-01, -2.6208e-01, -9.0910e-01,\n",
       "                      -3.2332e-01,  1.8217e-01, -1.0937e-01,  1.7382e-03, -1.1438e-01,\n",
       "                       9.9677e-02, -5.5168e-02, -3.7206e-02, -3.1858e-02,  2.4860e-01,\n",
       "                      -9.0162e-02, -4.2772e-01,  4.4220e-01, -3.4975e-01, -3.7338e-01,\n",
       "                       3.8119e-01,  1.2188e-02,  1.9269e-01,  1.1024e-01,  6.0619e-02,\n",
       "                      -7.8076e-03,  4.7894e-01,  1.0140e-01,  5.8977e-01,  8.2898e-02,\n",
       "                      -2.9091e-02, -8.7076e-02,  1.0226e-01,  2.9142e-01,  1.3744e-01,\n",
       "                       2.4098e-01,  3.7686e-01,  5.3487e-02,  5.0157e-01,  3.3295e-01,\n",
       "                       3.1061e-01,  1.6227e-01,  1.8316e-01,  1.0458e-02,  6.9072e-02,\n",
       "                      -1.3998e-02,  1.6098e-01,  6.5758e-01,  4.2681e-02,  1.1853e-01,\n",
       "                       1.1817e-01,  5.7244e-02,  1.0492e-01,  1.0618e-01,  1.9151e-01,\n",
       "                      -2.8725e-02,  1.6661e-01,  3.1668e-01,  8.5144e-02, -2.9331e-02,\n",
       "                       1.8039e-01,  2.7193e-01,  1.7292e-02,  1.5663e-01,  5.1350e-02,\n",
       "                       1.2558e-01,  2.0284e-01, -3.4176e-02,  7.6450e-02,  7.4655e-02,\n",
       "                       1.5043e-01,  3.0697e-02,  6.8139e-02, -1.7741e-02,  5.8584e-01,\n",
       "                       5.1581e-02, -2.3736e-02, -2.2242e-02, -5.8702e-03, -5.5032e-02,\n",
       "                       9.1215e-02, -1.2638e-01,  2.7725e-02,  2.2528e-01,  6.5504e-02,\n",
       "                       3.2373e-04,  2.9756e-01,  1.4441e-01,  3.3175e-01,  4.6318e-02,\n",
       "                       3.7642e-01,  1.1006e-01,  9.0361e-02,  3.3415e-02,  1.4524e-01,\n",
       "                       1.9052e-01,  1.9567e-01,  9.6544e-02,  6.3902e-01,  4.5474e-02,\n",
       "                       9.5381e-02,  3.8646e-01,  1.7836e-01,  1.9255e-01,  1.4137e-01,\n",
       "                       1.1966e-02,  4.5279e-01,  1.1574e-01,  2.2639e-01,  1.8484e-01,\n",
       "                       1.0201e-01,  1.7597e-02,  1.1894e-01,  6.6275e-01,  1.2593e-01,\n",
       "                       9.2465e-02,  1.7468e-01,  5.4408e-02,  1.1594e-01,  7.9836e-02,\n",
       "                       2.4622e-01,  3.1445e-01,  2.5278e-01, -2.0796e-02,  1.0581e-01,\n",
       "                       2.2754e-01,  2.1362e-01,  1.1156e-01, -1.0616e-01,  2.2312e-01,\n",
       "                       2.4400e-01, -1.7235e-02,  4.8690e-02,  2.1086e-01,  3.0073e-01,\n",
       "                       1.1443e-01,  1.1692e-02, -9.7899e-02,  4.5015e-01,  1.3264e-01,\n",
       "                       2.5789e-01, -4.0344e-02,  1.7059e-01,  1.6385e-01,  1.6031e-01,\n",
       "                       1.0059e-01,  1.9935e-01,  2.5260e-01,  7.1335e-02,  1.0437e-01])),\n",
       "             ('lstm.weight_ih_l1',\n",
       "              tensor([[ 0.0829,  0.1540, -0.0896,  ...,  0.1165,  0.1466, -0.2970],\n",
       "                      [ 0.0489,  0.1027,  0.1424,  ...,  0.0453,  0.0582, -0.1960],\n",
       "                      [-0.1060, -0.3526, -0.3613,  ...,  0.0393, -0.3643, -0.1196],\n",
       "                      ...,\n",
       "                      [-0.1778, -0.0543, -0.0566,  ..., -0.1493, -0.2687,  0.4125],\n",
       "                      [-0.1228,  0.3062,  0.0553,  ...,  0.4131, -0.1400,  0.0915],\n",
       "                      [-0.1247, -0.0674, -0.1336,  ...,  0.0973, -0.0368, -0.0022]])),\n",
       "             ('lstm.weight_hh_l1',\n",
       "              tensor([[ 0.2066,  0.0307,  0.0765,  ...,  0.1405, -0.1019,  0.0837],\n",
       "                      [ 0.1559, -0.0885, -0.4647,  ..., -0.0750,  0.1400, -0.1584],\n",
       "                      [-0.0511, -0.0084, -0.0792,  ..., -0.1477, -0.0518,  0.0890],\n",
       "                      ...,\n",
       "                      [-0.1029, -0.1408,  0.1740,  ...,  0.1159,  0.1212, -0.1580],\n",
       "                      [ 0.2120, -0.0344, -0.1394,  ...,  0.0284, -0.2679,  0.0620],\n",
       "                      [-0.2317, -0.0761, -0.0317,  ...,  0.1361, -0.1410,  0.0091]])),\n",
       "             ('lstm.bias_ih_l1',\n",
       "              tensor([-7.0969e-02, -4.7055e-02,  3.9398e-01,  7.0098e-02, -3.5563e-02,\n",
       "                       1.7337e-01,  8.3997e-02,  1.4005e-01,  1.3583e-01, -1.9619e-02,\n",
       "                       6.1650e-02, -9.7774e-02,  1.0400e-02,  8.8221e-02,  4.7013e-01,\n",
       "                      -2.8551e-02, -1.4986e-01,  2.2484e-01, -1.5296e-01,  4.6827e-02,\n",
       "                      -1.3042e-01,  1.8948e-01,  1.9331e-01, -1.0735e-01, -2.6108e-02,\n",
       "                       8.4733e-02,  3.7268e-01,  1.8100e-01,  2.1783e-01,  8.5924e-02,\n",
       "                      -5.7641e-02,  2.1618e-01,  5.1393e-03,  1.8125e-01,  9.2598e-02,\n",
       "                      -5.5966e-02,  9.4517e-02,  1.0632e-01, -5.4755e-02,  1.2104e-03,\n",
       "                       1.8568e-01,  3.4184e-01, -7.1506e-02, -1.5007e-01, -6.7435e-02,\n",
       "                       1.2974e-01,  1.8317e-01,  2.7069e-01, -2.9163e-02, -3.3143e-02,\n",
       "                      -6.0317e-02,  1.0782e-01,  1.7136e-01, -3.2046e-02,  1.7942e-01,\n",
       "                       3.6467e-01,  4.4421e-02,  6.4378e-02,  6.6222e-02,  8.2181e-02,\n",
       "                       1.9149e-01, -7.8664e-02, -2.1367e-01,  9.6557e-02,  2.6142e-02,\n",
       "                      -9.1557e-03,  1.6596e-02,  1.1839e-02, -6.9857e-02, -8.6715e-02,\n",
       "                       8.6744e-02, -6.5822e-02,  1.6534e-01,  4.0798e-02,  3.1041e-01,\n",
       "                      -1.4482e-02, -1.6894e-02,  1.7325e-02, -6.9531e-03, -2.6639e-03,\n",
       "                       1.4650e-01,  1.8510e-01, -1.7091e-02,  2.5966e-01,  2.9448e-01,\n",
       "                       6.2835e-02, -1.2953e-01, -5.6982e-03, -1.1204e-01,  2.8140e-02,\n",
       "                      -7.2547e-03, -1.8555e-02,  1.9881e-01,  7.4719e-02, -2.2622e-02,\n",
       "                      -3.6762e-02,  2.4577e-01,  1.5334e-01, -5.3983e-02, -1.1158e-01,\n",
       "                      -1.8758e-02,  1.2279e-01,  3.1097e-02,  1.6029e-01,  1.2339e-01,\n",
       "                       4.6950e-02,  1.7759e-01,  1.7948e-01,  8.1833e-02,  7.5498e-02,\n",
       "                       1.3356e-01,  3.2790e-02, -3.3587e-02,  1.7407e-01,  2.9394e-01,\n",
       "                       5.2535e-02,  3.3728e-01, -3.4258e-02,  5.3409e-02, -2.0383e-01,\n",
       "                       2.8292e-01,  1.3823e-01,  8.2646e-02, -3.8366e-02,  1.7650e-01,\n",
       "                      -3.0214e-02,  1.4641e-01, -2.6753e-02,  1.9666e-01,  1.1173e-01,\n",
       "                      -7.2422e-02,  4.8448e-03, -3.3658e-02, -2.1934e-02, -4.1920e-02,\n",
       "                      -1.1534e-01,  1.6456e-03, -4.4535e-03,  8.5148e-02, -9.1572e-03,\n",
       "                      -9.1303e-02, -3.4651e-02,  6.8457e-02, -1.3625e-01, -4.7238e-02,\n",
       "                      -6.0475e-02, -2.0909e-02,  8.1431e-02, -9.5612e-03, -6.1917e-02,\n",
       "                       4.8878e-02,  1.8432e-02,  5.0807e-02,  2.4977e-02, -5.6425e-03,\n",
       "                      -5.1393e-02, -5.5061e-02, -5.1140e-02, -5.0939e-02, -6.2659e-02,\n",
       "                       4.9577e-02,  2.3989e-02,  3.6079e-02,  9.9226e-03, -5.0606e-02,\n",
       "                       5.5657e-02, -7.8953e-02, -4.8456e-02, -3.9937e-02,  3.2588e-02,\n",
       "                       1.9574e-02, -4.2012e-02,  5.1684e-02, -8.9347e-03,  3.4367e-02,\n",
       "                      -4.1233e-02, -9.8205e-02,  1.3711e-02, -4.1476e-02,  4.7400e-02,\n",
       "                       3.9246e-02, -2.7136e-02, -4.9067e-02, -1.3002e-02,  5.4379e-04,\n",
       "                      -7.3958e-02,  6.4205e-02, -3.1421e-02,  2.5186e-02,  1.2011e-02,\n",
       "                      -7.2628e-02, -6.4986e-02, -4.4429e-02, -5.0932e-03,  8.9988e-02,\n",
       "                      -5.9112e-02, -6.8777e-02,  8.5121e-02, -6.7039e-02,  3.0886e-02,\n",
       "                       2.8383e-02, -2.3584e-02, -3.9682e-02,  3.3800e-02, -7.0983e-02,\n",
       "                      -3.4056e-02, -3.8089e-02,  3.4558e-02,  6.5130e-02, -1.4246e-02,\n",
       "                       2.2859e-02, -1.1135e-02, -4.9946e-02, -4.1579e-02, -3.8541e-02,\n",
       "                      -9.4072e-04, -1.4544e-01, -1.0114e-01,  4.8415e-02,  4.4266e-02,\n",
       "                      -8.8534e-02, -6.6388e-02, -4.1072e-02, -2.3371e-03, -2.4657e-02,\n",
       "                       9.8105e-02,  2.3592e-02, -1.7716e-01, -1.5174e-02, -7.8341e-02,\n",
       "                      -4.3083e-02,  5.4371e-02, -4.0025e-02,  7.3985e-02,  2.9883e-02,\n",
       "                      -3.8400e-02, -3.2419e-02, -2.9748e-02,  2.0720e-02, -4.0723e-02,\n",
       "                       5.4036e-03, -5.1501e-02, -7.5006e-03, -3.8588e-02, -2.7622e-03,\n",
       "                       3.2665e-03,  7.4883e-02,  1.7587e-02, -4.5068e-02, -1.2918e-01,\n",
       "                      -9.6298e-02, -1.4785e-02, -7.9216e-02, -8.4544e-03, -7.7606e-02,\n",
       "                       1.0039e-01,  3.7193e-02, -1.3533e-02,  4.0625e-02, -8.3335e-02,\n",
       "                       4.1790e-02,  1.5046e-02, -3.0863e-01,  2.9853e-01, -6.9201e-02,\n",
       "                      -8.6158e-02, -7.3676e-02, -1.2103e-01,  3.1512e-01,  8.3907e-02,\n",
       "                       5.7745e-02, -7.0320e-02, -1.2098e-02, -6.3337e-03,  7.6310e-02,\n",
       "                      -2.0632e-02,  3.1492e-02, -8.9678e-02,  2.4267e-02, -8.2530e-02,\n",
       "                      -6.0358e-03, -7.4312e-02,  6.6595e-03, -2.0283e-02, -1.0134e-01,\n",
       "                       3.6317e-02,  6.8682e-02, -5.6537e-02,  1.9089e-02, -7.4277e-02,\n",
       "                       1.0187e-01,  1.0730e-01, -9.1562e-02,  3.7849e-02,  6.0477e-02,\n",
       "                      -1.0638e-02, -4.1002e-02, -2.8102e-02, -1.7970e-02,  1.7435e-01,\n",
       "                      -1.0842e-01,  9.3804e-03, -4.1860e-02,  7.9891e-03,  1.0897e-02,\n",
       "                       1.6944e-03, -1.5660e-01, -2.0217e-01,  1.7077e-02, -1.0953e-03,\n",
       "                      -7.5249e-02,  6.1843e-02,  7.6320e-02, -1.9807e-03,  7.3341e-02,\n",
       "                      -8.6387e-02, -1.0985e-01,  1.1207e-01, -5.3926e-02, -1.6516e-01,\n",
       "                       2.3437e-02, -1.5624e-01, -4.7742e-02, -7.2538e-02, -4.2457e-02,\n",
       "                       7.7320e-02,  5.5651e-02,  1.0386e-01,  6.3666e-02,  4.0117e-02,\n",
       "                      -6.2635e-02, -7.6129e-02, -2.0860e-02,  2.9882e-02, -3.4369e-02,\n",
       "                       7.8089e-02,  3.5800e-02, -5.8412e-02, -1.9706e-01,  4.2979e-02,\n",
       "                      -8.7240e-02,  1.9485e-02,  3.0875e-03,  1.2988e-01, -2.8514e-02,\n",
       "                      -1.8333e-01, -4.6971e-02, -2.0546e-02, -2.4148e-02,  7.6566e-02,\n",
       "                      -4.0053e-03,  5.5292e-02, -1.9443e-01,  2.5187e-02, -2.9516e-02,\n",
       "                      -8.7925e-03, -1.1587e-01, -7.4446e-02, -2.9080e-03, -5.6520e-02,\n",
       "                       9.2216e-02, -9.6096e-02, -6.6439e-02, -5.9185e-02, -7.2689e-02,\n",
       "                       7.4337e-03, -1.6369e-01,  4.0548e-03, -1.2957e-02, -4.0659e-02,\n",
       "                       1.5787e-01, -2.1478e-02,  1.1684e-01, -1.2161e-02,  8.7785e-02,\n",
       "                      -1.2838e-01, -1.1317e-01,  1.2729e-04,  9.9238e-03,  3.3596e-02,\n",
       "                      -1.5866e-01,  1.1942e-01, -3.5838e-02,  2.8090e-02,  4.7269e-03,\n",
       "                      -7.1833e-02,  8.4086e-02,  1.0330e-01, -8.5749e-02,  4.6189e-02,\n",
       "                       2.5915e-02, -6.3301e-02,  1.2601e-01,  5.5080e-02,  1.1733e-01,\n",
       "                       1.5278e-01,  1.6620e-03,  3.8442e-02, -3.4513e-02,  6.4369e-02,\n",
       "                      -5.5676e-04,  4.7382e-03,  7.8325e-02,  1.1948e-01,  4.1146e-01,\n",
       "                      -1.1451e-02,  3.4547e-02,  1.2278e-01,  1.0825e-01,  5.2904e-02,\n",
       "                       2.8835e-02,  1.4267e-01,  1.3855e-01, -5.8087e-02,  4.1119e-02,\n",
       "                       6.8206e-02,  3.8608e-01,  9.9131e-02,  3.3336e-02,  1.2751e-01,\n",
       "                      -4.7645e-02,  7.0241e-02,  1.3092e-01,  9.7603e-02, -2.8304e-02,\n",
       "                       1.2352e-01, -4.0770e-02,  9.3263e-02,  1.3313e-02,  9.5866e-02,\n",
       "                       4.0898e-03,  2.3815e-01,  1.7797e-02, -1.0196e-01,  6.5764e-02,\n",
       "                       9.8770e-03,  1.4965e-01,  1.9866e-01,  8.0471e-02,  2.3354e-02,\n",
       "                      -3.2208e-02, -1.1862e-02,  1.2421e-01, -5.1049e-02,  1.4319e-01,\n",
       "                       1.5028e-01,  6.5235e-02,  4.8121e-02,  1.2360e-02,  7.5005e-02,\n",
       "                       1.5938e-02,  7.2528e-02, -1.5192e-02,  1.9617e-01,  5.7991e-02,\n",
       "                       2.3453e-02,  9.2448e-02, -4.6972e-02,  1.1854e-01,  4.2942e-02,\n",
       "                      -9.5726e-02, -5.2993e-02,  5.3903e-02,  4.7816e-02,  2.2905e-01,\n",
       "                      -7.4076e-02, -9.9510e-02,  3.9710e-02,  3.2609e-04,  7.8302e-02,\n",
       "                       1.9466e-02,  3.6651e-02,  5.0846e-02,  5.5656e-02,  2.7467e-01,\n",
       "                       2.0653e-02, -4.7368e-02, -1.6845e-02,  3.3413e-02,  1.2410e-01,\n",
       "                       5.8362e-02,  6.5336e-02,  4.0722e-02,  1.3994e-02,  5.5303e-02,\n",
       "                       5.4245e-02,  2.8196e-01,  3.2234e-01, -1.3519e-01, -3.9005e-02,\n",
       "                       3.4871e-02,  1.3356e-01,  2.4343e-02,  9.0574e-02, -8.0630e-02,\n",
       "                       3.1048e-02,  4.8117e-03,  7.3126e-02,  1.0474e-01,  6.5915e-02,\n",
       "                       2.5427e-02,  2.8663e-02,  2.7983e-02,  4.0202e-02,  1.1862e-01,\n",
       "                       4.2767e-03,  8.8491e-02,  2.3158e-02,  9.0023e-03, -2.9835e-02,\n",
       "                       8.8600e-02,  1.3649e-01,  2.9819e-02, -8.2845e-02,  6.5456e-02,\n",
       "                       1.0168e-01,  6.7576e-02,  3.2039e-02,  4.7474e-02,  2.6444e-01])),\n",
       "             ('lstm.bias_hh_l1',\n",
       "              tensor([-1.7337e-01, -3.2773e-02,  3.3469e-01,  5.3904e-02,  6.8181e-02,\n",
       "                       2.1563e-01,  8.5164e-02,  9.9294e-02,  1.3507e-01, -3.3085e-02,\n",
       "                       1.0855e-02,  1.3625e-02,  2.0507e-02,  1.4164e-01,  5.0370e-01,\n",
       "                      -4.3662e-02, -7.5935e-02,  3.2472e-01, -3.6034e-02,  7.4167e-02,\n",
       "                      -1.1252e-01,  2.3472e-01,  2.2299e-01, -6.2753e-02, -4.6512e-02,\n",
       "                       3.6801e-02,  3.7076e-01,  1.2890e-01,  2.7292e-01,  5.3351e-02,\n",
       "                      -6.3343e-02,  2.7995e-01,  8.5367e-02,  1.5879e-01, -2.4065e-02,\n",
       "                      -1.2617e-01,  5.5319e-02,  1.1176e-01, -4.7109e-03, -4.6844e-02,\n",
       "                       1.4492e-01,  3.8694e-01, -5.6573e-02, -8.5619e-02,  2.9031e-02,\n",
       "                       9.4250e-02,  1.7976e-01,  2.5636e-01,  8.1905e-02, -1.0987e-02,\n",
       "                       1.4568e-02,  6.9982e-02,  1.6443e-01, -5.7931e-02,  1.6411e-01,\n",
       "                       3.3724e-01,  1.9873e-02, -5.0955e-02,  8.8677e-02,  1.8026e-01,\n",
       "                       1.5095e-01, -2.0515e-02, -2.2839e-01,  1.6692e-01,  3.6541e-02,\n",
       "                       2.8644e-02,  1.4462e-01, -1.5787e-03, -8.0077e-02,  1.1485e-02,\n",
       "                       2.1205e-02,  4.5133e-02,  1.9220e-01, -2.9750e-02,  4.2258e-01,\n",
       "                      -1.0943e-01, -9.7187e-02,  5.9934e-02, -5.9174e-03, -6.2819e-02,\n",
       "                       7.8269e-02,  1.3667e-01, -6.9386e-02,  1.8240e-01,  2.4788e-01,\n",
       "                       1.1930e-01,  1.2277e-02, -9.3600e-02,  8.0400e-03,  8.8514e-02,\n",
       "                       5.8731e-02,  7.9015e-02,  2.2558e-01,  1.5818e-01,  5.0321e-02,\n",
       "                       4.9769e-02,  2.3188e-01,  1.5172e-01, -5.6863e-03, -1.8412e-01,\n",
       "                       3.9623e-02,  1.7965e-01, -2.1463e-02,  2.4439e-01,  2.8419e-02,\n",
       "                       2.7902e-02,  2.8688e-01,  1.1527e-01,  1.2219e-01,  6.2061e-02,\n",
       "                       1.7191e-01, -8.8550e-02, -9.5337e-02,  1.4562e-01,  1.9982e-01,\n",
       "                      -1.5394e-02,  3.0588e-01, -5.9921e-02,  4.3566e-02, -9.7563e-02,\n",
       "                       2.8252e-01,  2.1704e-01,  1.4752e-01, -3.4805e-02,  2.1798e-01,\n",
       "                       5.8846e-02,  8.2020e-02, -4.3642e-02,  1.7613e-01,  9.5514e-02,\n",
       "                       1.2455e-02,  3.1171e-02,  8.5121e-03,  8.1302e-02, -3.7144e-02,\n",
       "                      -1.2269e-01, -6.5093e-02, -1.9019e-02,  1.1685e-01, -2.0716e-02,\n",
       "                      -7.0234e-02, -5.0847e-02,  4.7897e-02, -3.1111e-02, -1.1476e-01,\n",
       "                      -2.6558e-02,  7.3651e-03,  7.7472e-02, -4.9145e-02, -3.0634e-02,\n",
       "                      -2.0384e-02, -2.7523e-02,  2.9863e-02, -7.5423e-02,  2.5590e-02,\n",
       "                      -1.0495e-01, -5.6229e-02, -5.3368e-02, -7.4607e-03,  9.0569e-03,\n",
       "                       5.0803e-02, -2.7059e-03, -5.9295e-02,  2.9734e-02,  1.0405e-02,\n",
       "                       4.7638e-02, -7.7119e-02, -1.4762e-01, -3.6833e-02,  9.0780e-03,\n",
       "                       4.4492e-02, -7.9721e-02, -3.9763e-02, -2.5860e-02,  1.4642e-02,\n",
       "                       6.8090e-03, -4.0367e-02,  4.8503e-02,  6.6044e-02, -2.1339e-02,\n",
       "                      -4.4563e-02,  2.6961e-02, -2.8068e-02,  1.9423e-02,  3.4610e-02,\n",
       "                      -9.6676e-02,  8.8070e-02,  1.7034e-02,  2.2476e-02, -8.3124e-02,\n",
       "                      -7.7294e-02,  3.9249e-02,  8.4331e-02,  2.5049e-02,  2.3322e-02,\n",
       "                      -6.6040e-02, -5.0840e-02, -7.0282e-04, -2.8493e-02,  4.6176e-02,\n",
       "                       9.1003e-03,  3.9115e-02,  2.5731e-03, -1.0641e-01, -1.8344e-01,\n",
       "                       7.6485e-02,  4.2858e-02, -1.9411e-02,  7.5805e-02, -4.9238e-02,\n",
       "                      -3.2453e-03,  6.8681e-04,  1.1261e-02, -1.4025e-01, -6.8991e-02,\n",
       "                      -4.3654e-03, -2.8335e-02, -1.0494e-01,  3.5889e-03,  5.6038e-02,\n",
       "                      -8.7900e-02, -7.0393e-02, -1.0868e-01,  7.4207e-03, -2.8666e-02,\n",
       "                      -2.7263e-02,  4.9183e-02, -8.3951e-02,  1.2052e-02, -1.2451e-01,\n",
       "                      -2.6864e-02,  6.2515e-02, -5.0162e-02, -3.7540e-02, -6.3698e-02,\n",
       "                       4.9486e-02,  9.5635e-03, -3.4903e-02,  1.3902e-02,  3.1721e-02,\n",
       "                      -4.2839e-02, -4.8101e-02, -2.7492e-02, -6.8208e-02,  2.8976e-02,\n",
       "                      -4.7788e-02,  9.9618e-02, -5.2091e-02,  3.6261e-02, -1.1253e-02,\n",
       "                       3.4105e-03, -2.0186e-03, -2.8107e-02, -3.7427e-02, -6.3588e-02,\n",
       "                       6.3686e-02, -7.4858e-02, -2.6020e-02,  6.7693e-02,  3.3861e-02,\n",
       "                       3.0004e-02,  8.4350e-02, -2.4442e-01,  1.7036e-01,  2.2586e-02,\n",
       "                      -1.1565e-01, -4.5344e-02, -1.5625e-02,  3.2418e-01,  1.1543e-01,\n",
       "                       5.3606e-02,  1.9996e-02,  6.8349e-02,  3.2982e-02,  9.4375e-02,\n",
       "                      -3.8294e-02, -2.6562e-02, -1.8795e-02,  5.6151e-02, -1.6063e-01,\n",
       "                      -7.2668e-02,  2.6026e-02,  3.8294e-02,  3.6401e-02, -5.9218e-02,\n",
       "                       8.1049e-03,  8.3829e-02,  1.6651e-02,  1.3443e-01, -5.2200e-02,\n",
       "                       1.0992e-01,  8.5735e-02, -4.9075e-02,  1.0664e-01,  1.2100e-01,\n",
       "                      -1.3172e-02, -3.5980e-02, -1.3411e-01,  8.0088e-02,  1.6164e-01,\n",
       "                      -2.2565e-02, -1.5250e-02, -1.4437e-02, -1.0354e-02, -5.9391e-02,\n",
       "                      -4.5402e-02, -1.3629e-01, -9.9100e-02, -3.1587e-02,  1.1085e-01,\n",
       "                      -1.5886e-01,  1.4862e-01,  3.8227e-02, -8.7104e-02, -2.9397e-02,\n",
       "                      -1.2491e-01,  2.2335e-02,  9.7113e-02, -5.9962e-02, -6.1414e-02,\n",
       "                       2.4391e-02, -2.3349e-02,  5.2947e-02, -1.0400e-01, -1.6349e-02,\n",
       "                      -1.4308e-02,  9.9085e-03,  8.6140e-02,  2.8100e-02,  8.6982e-02,\n",
       "                      -1.4655e-04, -7.3617e-02, -3.2527e-02, -2.6441e-04, -2.5651e-02,\n",
       "                      -3.1905e-02,  2.4139e-02,  2.1940e-02, -1.5700e-01, -2.9398e-02,\n",
       "                      -7.6008e-02, -1.9222e-02,  2.9418e-02,  4.7029e-02,  2.4893e-02,\n",
       "                      -6.5907e-02,  4.8441e-02, -5.9197e-02, -1.1685e-01,  1.2306e-02,\n",
       "                       5.1783e-02,  7.1364e-02, -6.0037e-02,  3.4483e-02, -5.9971e-02,\n",
       "                       1.1471e-01, -1.4914e-01, -1.4541e-01, -1.0453e-02, -4.3513e-02,\n",
       "                       7.0144e-02, -1.9372e-02, -1.1022e-01, -9.0237e-02, -3.7537e-02,\n",
       "                       4.6737e-02, -5.1378e-02,  7.5605e-02,  5.2580e-02,  5.3556e-02,\n",
       "                       6.5673e-02, -1.1316e-01,  2.8805e-02, -7.0882e-02,  8.3750e-02,\n",
       "                      -1.1339e-01, -2.1586e-01, -2.9123e-02, -3.5210e-03,  3.3249e-03,\n",
       "                      -1.9399e-01,  6.9561e-02,  2.2323e-02,  4.8141e-02,  4.1058e-02,\n",
       "                       4.3300e-02,  9.0458e-02,  3.2306e-02, -5.9669e-02,  1.5384e-02,\n",
       "                       3.4460e-02, -2.7187e-02,  8.6605e-02,  1.0983e-01,  1.9974e-02,\n",
       "                       1.3498e-01,  6.0731e-02,  2.5011e-02, -4.3357e-02,  1.1436e-01,\n",
       "                       8.8012e-03, -3.9276e-02,  7.6740e-02,  4.9521e-02,  4.1698e-01,\n",
       "                      -2.9814e-02,  4.3517e-02,  1.1065e-01,  2.7562e-02,  7.2686e-03,\n",
       "                      -5.2263e-02,  6.2251e-02,  4.4361e-02, -4.5440e-03, -2.8698e-02,\n",
       "                       8.9071e-02,  4.2208e-01,  1.4133e-01,  1.3666e-02,  6.9994e-02,\n",
       "                      -1.9295e-02,  1.6989e-02,  9.8115e-02,  3.6710e-02, -4.2362e-02,\n",
       "                       4.7403e-02,  4.9329e-02,  1.0035e-01, -2.3155e-02,  9.4142e-02,\n",
       "                       1.9012e-02,  3.2999e-01, -1.1262e-01, -6.5549e-02,  3.4334e-02,\n",
       "                       4.5356e-03,  2.3556e-01,  1.4862e-01,  4.8976e-02, -5.6625e-02,\n",
       "                      -5.5152e-02,  3.9753e-02,  1.1947e-01, -4.5276e-02,  8.3760e-02,\n",
       "                       4.4587e-02,  1.2628e-01, -2.4662e-02,  9.8048e-02,  1.1383e-01,\n",
       "                       1.9847e-02, -2.7283e-03, -1.2275e-01,  1.8093e-01,  7.5144e-02,\n",
       "                      -4.1180e-03,  4.2733e-02, -8.0037e-02,  6.3368e-02, -3.7123e-02,\n",
       "                      -9.9708e-02,  5.6909e-02,  8.9159e-03, -5.6408e-02,  2.8715e-01,\n",
       "                      -5.6545e-03, -7.6076e-03,  8.2869e-02, -6.6091e-03,  2.2317e-02,\n",
       "                       5.5713e-02, -8.1219e-02, -5.3996e-02,  5.7151e-02,  2.4010e-01,\n",
       "                       1.9413e-02, -9.1455e-04, -5.8859e-02,  3.1165e-02,  1.1954e-01,\n",
       "                       8.6507e-02, -1.6519e-03,  1.1149e-02,  7.5129e-02,  6.1786e-02,\n",
       "                      -6.5507e-02,  2.4967e-01,  4.5055e-01, -3.8194e-02,  3.0292e-02,\n",
       "                       2.9740e-02,  1.0834e-02, -5.4068e-02,  1.2069e-01, -6.8077e-03,\n",
       "                      -3.6643e-02,  3.7733e-02,  4.9019e-02,  1.0219e-01, -3.4486e-03,\n",
       "                      -3.2358e-02,  1.1020e-03, -1.3601e-02,  4.4113e-02,  1.1082e-01,\n",
       "                      -4.2781e-02,  4.0650e-02,  9.6750e-02, -4.2745e-02, -3.3144e-02,\n",
       "                       5.8472e-02,  1.0375e-01, -2.8205e-02, -5.8232e-02,  1.0267e-01,\n",
       "                       1.0452e-01,  2.2379e-02, -4.9141e-02,  8.2986e-02,  1.4499e-01])),\n",
       "             ('lstm.weight_ih_l2',\n",
       "              tensor([[ 0.0413,  0.0609, -0.3064,  ...,  0.0575,  0.1086,  0.1138],\n",
       "                      [ 0.0216, -0.2733,  0.1505,  ..., -0.0523,  0.1374,  0.1935],\n",
       "                      [-0.3848, -0.1135, -0.2710,  ...,  0.2334,  0.2578,  0.3851],\n",
       "                      ...,\n",
       "                      [-0.4147, -0.2904,  0.6799,  ..., -0.2615,  0.3045,  0.2494],\n",
       "                      [ 0.3343, -0.1726,  0.3554,  ..., -0.0779,  0.2703,  1.0022],\n",
       "                      [-0.1221, -0.2937,  0.3023,  ...,  0.0167, -0.0435,  0.2110]])),\n",
       "             ('lstm.weight_hh_l2',\n",
       "              tensor([[ 0.0878,  0.1517,  0.0041,  ..., -0.0889,  0.0217, -0.1120],\n",
       "                      [ 0.6132, -0.0778,  0.2543,  ..., -0.2993, -0.4360,  0.1420],\n",
       "                      [-0.3051, -0.0817, -0.0385,  ..., -0.1876,  0.3007,  0.3028],\n",
       "                      ...,\n",
       "                      [ 0.2689, -0.0088, -0.0079,  ...,  0.0157, -0.0110, -0.2303],\n",
       "                      [-0.2006,  0.2478, -0.2718,  ..., -0.0314,  0.3607,  0.1206],\n",
       "                      [ 0.0324, -0.1298, -0.0448,  ...,  0.0634,  0.1261,  0.2446]])),\n",
       "             ('lstm.bias_ih_l2',\n",
       "              tensor([ 2.3318e-02,  6.7953e-02,  7.1804e-02,  6.1229e-02,  6.6382e-02,\n",
       "                       2.6799e-02,  5.4862e-02, -2.6090e-02,  2.2726e-02,  1.0862e-01,\n",
       "                       1.6615e-01, -8.4488e-03, -5.3615e-02,  2.0116e-02,  5.7892e-02,\n",
       "                       1.0235e-01,  1.7022e-02,  2.2202e-02,  6.4058e-02,  2.6125e-02,\n",
       "                       2.0136e-01, -4.8841e-02,  3.2073e-02,  2.9218e-02,  1.0380e-01,\n",
       "                       1.3346e-01,  4.8743e-02,  2.8686e-02,  1.8189e-01,  3.1843e-02,\n",
       "                       2.6821e-01, -1.2233e-02,  2.3848e-02,  4.7095e-02,  1.0082e-01,\n",
       "                      -2.7156e-02,  1.5510e-01, -3.8824e-02,  4.2439e-02,  6.0937e-02,\n",
       "                       8.2297e-02,  2.8308e-02,  1.0248e-01,  8.8624e-02,  8.3095e-03,\n",
       "                       6.5187e-02,  1.0027e-01, -7.4731e-02,  4.9339e-02, -2.9242e-02,\n",
       "                       4.6656e-02, -1.0863e-02,  6.4956e-02,  4.0442e-02,  1.5057e-01,\n",
       "                      -3.7275e-04, -8.3859e-02,  4.0709e-02,  1.7068e-01,  1.1931e-01,\n",
       "                      -5.9330e-02, -2.5083e-02,  8.9687e-02,  7.4242e-02,  4.1211e-01,\n",
       "                       7.4593e-02,  1.9753e-02,  6.4733e-02,  2.7779e-01,  9.9072e-02,\n",
       "                       2.7371e-02, -1.6112e-01,  3.9896e-02,  3.2035e-01,  1.4657e-01,\n",
       "                       1.6840e-01, -7.7668e-02, -5.7504e-02,  1.7018e-02, -2.0229e-02,\n",
       "                       7.4090e-02,  2.7388e-02,  8.4612e-02,  1.4240e-01,  1.1700e-01,\n",
       "                      -3.3874e-02, -7.4899e-03,  5.8788e-02,  8.8460e-03, -1.4753e-01,\n",
       "                       4.9303e-02,  5.8196e-02, -1.3833e-02,  1.3799e-02,  6.7955e-02,\n",
       "                      -6.9584e-02,  7.4574e-02, -8.0211e-02,  1.1636e-01, -4.7240e-02,\n",
       "                      -3.5993e-02,  3.3336e-02,  8.6617e-02,  3.0816e-02,  1.1959e-01,\n",
       "                       1.6460e-01,  4.7533e-02,  2.2002e-01,  2.6625e-02,  6.2919e-03,\n",
       "                       1.4226e-01,  3.4189e-02,  1.3558e-01,  1.9786e-02,  5.1731e-02,\n",
       "                      -4.9799e-02,  1.4704e-02, -5.2608e-02, -1.4769e-02, -2.4339e-03,\n",
       "                       8.3080e-02,  1.4038e-01,  4.9083e-02,  7.1733e-02,  1.7085e-02,\n",
       "                       8.1374e-02,  8.6110e-02,  3.8944e-02,  1.4111e-01,  1.0711e-02,\n",
       "                      -5.6051e-02,  2.6912e-02,  4.8073e-02, -3.7741e-02, -9.9929e-02,\n",
       "                      -1.6189e-01, -1.0618e-02, -8.8733e-03,  2.6524e-02, -7.3925e-02,\n",
       "                      -8.3918e-02,  1.0462e-01, -4.0243e-03, -3.0029e-02,  2.7641e-02,\n",
       "                      -6.8528e-02,  6.5875e-02, -1.5364e-03,  4.1032e-02,  3.2714e-03,\n",
       "                       7.4358e-02, -1.0167e-01,  3.4982e-02, -3.1465e-02,  3.0461e-02,\n",
       "                      -1.2740e-02, -3.0739e-02,  5.9000e-02,  2.6334e-02,  2.6681e-02,\n",
       "                       5.0127e-02, -8.0687e-02,  1.4649e-02, -7.6120e-02, -3.7679e-02,\n",
       "                      -8.1567e-02, -3.0877e-02,  6.0044e-02, -3.9071e-02, -5.9984e-02,\n",
       "                      -4.8350e-02,  6.0094e-02, -2.2345e-02,  7.9234e-02, -3.1611e-02,\n",
       "                       2.7613e-02, -1.1325e-01, -1.1295e-01, -3.0620e-02, -8.4150e-02,\n",
       "                      -2.9380e-02,  6.1742e-02, -5.7974e-02, -1.3776e-02, -9.7163e-03,\n",
       "                       2.7528e-02,  9.2002e-02, -7.5516e-02,  1.1911e-01,  1.4228e-02,\n",
       "                      -2.7987e-02, -4.0531e-02, -7.8034e-02, -5.7070e-02,  5.8207e-02,\n",
       "                       1.9640e-02, -1.7452e-02,  2.5774e-02,  8.7482e-03, -1.4877e-02,\n",
       "                      -1.7568e-04,  2.3740e-03, -7.7607e-02,  2.0189e-02,  1.7664e-02,\n",
       "                       7.9608e-02,  5.9209e-03,  2.2623e-02, -3.2544e-02,  4.4212e-02,\n",
       "                      -4.5945e-02,  3.5220e-02, -2.1958e-02,  4.2966e-02,  2.4570e-02,\n",
       "                      -1.8478e-02,  4.9329e-02,  7.5041e-02,  7.3984e-02,  5.1049e-03,\n",
       "                      -5.3440e-03,  5.1528e-02,  2.4697e-02,  8.9805e-02, -4.8289e-02,\n",
       "                       7.8179e-02, -2.4809e-02, -4.4945e-02,  9.6653e-04, -9.0357e-02,\n",
       "                      -4.5528e-02,  6.1611e-02, -1.4744e-02,  1.8018e-03,  2.4558e-02,\n",
       "                       1.3868e-02,  2.1298e-02,  4.5768e-02,  3.8950e-03, -1.4709e-02,\n",
       "                      -3.7717e-03,  5.9654e-02,  6.5813e-03,  4.7820e-02, -4.1298e-02,\n",
       "                      -7.6512e-02, -1.4155e-02, -8.5084e-02,  1.2969e-02,  2.1470e-02,\n",
       "                      -3.6072e-02, -6.5593e-02,  4.6409e-02, -8.3214e-02,  2.7960e-02,\n",
       "                      -1.0620e-02, -1.4657e-02,  1.7720e-03, -3.0287e-02,  1.1516e-03,\n",
       "                       1.7608e-01, -4.8006e-02,  1.1519e-01,  5.6903e-04,  8.8337e-02,\n",
       "                       4.8557e-02, -5.0489e-02, -4.2139e-02, -3.1045e-02,  6.9544e-02,\n",
       "                      -4.9749e-02,  4.2934e-02,  2.1648e-02,  1.0092e-02, -2.5083e-02,\n",
       "                      -9.4074e-02, -4.3421e-02, -7.2460e-02,  1.9752e-01, -3.0980e-02,\n",
       "                       9.0094e-03,  1.7590e-02,  2.9558e-02, -7.2394e-03, -1.0882e-01,\n",
       "                       1.3197e-01,  1.4666e-01, -9.1018e-02,  5.4131e-02,  5.2810e-02,\n",
       "                       2.6444e-01, -1.1600e-01, -4.9700e-02, -1.0371e-01,  6.7644e-02,\n",
       "                       7.7727e-02, -2.1842e-01,  4.5856e-02, -1.1234e-01,  4.1540e-02,\n",
       "                      -5.1736e-02, -3.8124e-02,  2.8890e-02,  1.1948e-01,  9.0585e-02,\n",
       "                       1.1700e-01, -7.1004e-03, -9.1455e-02,  4.8731e-02, -1.0269e-01,\n",
       "                      -1.3572e-01, -7.9868e-02,  2.6726e-02, -9.6208e-02,  1.7355e-01,\n",
       "                      -1.2614e-01, -2.9169e-02,  7.4696e-02, -8.5038e-02, -2.4135e-02,\n",
       "                       5.9856e-02,  2.9668e-02,  8.9010e-02,  2.2530e-01, -2.4499e-01,\n",
       "                       2.5315e-02,  2.1177e-01,  2.3658e-02,  7.4104e-02, -1.3612e-01,\n",
       "                      -4.3933e-02,  5.2294e-02,  8.0052e-02,  3.8894e-01,  8.6281e-02,\n",
       "                       5.5580e-02,  3.7917e-02, -1.5161e-02, -1.4046e-01, -2.1246e-01,\n",
       "                       1.2863e-01,  1.6382e-02,  1.1970e-01, -1.1938e-01,  3.2891e-03,\n",
       "                       3.7057e-02,  1.7970e-02,  2.0325e-02, -1.3405e-02, -1.1449e-01,\n",
       "                      -6.2161e-02,  2.6374e-02,  9.7076e-02, -1.8404e-01,  2.1614e-02,\n",
       "                      -4.6592e-02,  1.2657e-01, -6.4700e-03,  1.8617e-02,  2.4009e-02,\n",
       "                      -3.2878e-03,  1.5325e-01, -2.1772e-01,  9.1452e-02,  1.9571e-02,\n",
       "                       1.9345e-02,  2.0799e-02, -1.4113e-01,  1.3369e-01,  1.5200e-01,\n",
       "                      -1.3906e-01,  1.0106e-01,  4.7876e-02,  6.3042e-03,  1.6466e-02,\n",
       "                      -6.5047e-02, -1.3020e-01, -1.6893e-01, -1.6440e-02,  3.0209e-02,\n",
       "                      -3.2692e-04,  1.3820e-01, -5.7705e-02, -1.4009e-01, -5.1670e-02,\n",
       "                       8.4509e-02, -2.5695e-02,  3.0334e-02, -2.0089e-01,  5.0392e-02,\n",
       "                      -9.3201e-02, -1.8917e-02, -5.0771e-02, -7.4114e-03,  6.4452e-02,\n",
       "                      -5.3530e-02,  9.2099e-02,  5.2421e-03,  2.8667e-02,  2.1657e-02,\n",
       "                       6.4018e-02, -3.1395e-02,  5.5322e-02,  2.8885e-02,  1.1386e-01,\n",
       "                       5.5562e-02, -1.2594e-02,  7.5399e-02, -7.0734e-02,  1.1785e-01,\n",
       "                       1.1278e-01, -2.2492e-02, -6.3996e-02, -2.7287e-02,  6.6620e-02,\n",
       "                      -1.5285e-02,  1.6195e-01, -6.3895e-02, -3.4121e-02, -9.6522e-02,\n",
       "                       1.1428e-01,  8.6690e-02,  1.6468e-02, -2.9856e-02,  3.9143e-02,\n",
       "                      -2.1214e-02, -1.2422e-02,  2.5234e-02, -1.9689e-02,  4.6714e-02,\n",
       "                       4.8859e-02, -6.5910e-03, -1.6845e-02,  5.9951e-02,  2.9948e-02,\n",
       "                       2.2867e-02,  1.1921e-02, -7.4383e-02, -1.2979e-02,  1.9396e-02,\n",
       "                       1.8826e-02, -7.3027e-02, -1.8417e-02, -3.6653e-02,  4.0541e-02,\n",
       "                      -6.5603e-02,  1.0033e-01,  8.9002e-04,  2.8674e-01,  9.6074e-02,\n",
       "                      -9.1469e-03,  4.0519e-02,  1.3349e-01,  4.1735e-02,  8.6583e-04,\n",
       "                       3.6775e-02,  3.2177e-02, -6.2739e-02, -5.5419e-02, -6.3741e-02,\n",
       "                       1.0249e-02,  4.7886e-02,  2.5189e-02,  8.7456e-02,  5.7200e-02,\n",
       "                       1.6975e-01, -7.8475e-03,  4.2230e-02,  4.0143e-02,  1.1609e-02,\n",
       "                       1.0074e-01,  6.1451e-02, -3.6150e-02,  4.9309e-02,  1.1959e-01,\n",
       "                       2.4111e-03, -8.0538e-02,  5.5643e-04, -4.2118e-03,  3.1942e-03,\n",
       "                      -4.4535e-02, -1.8319e-02,  1.6662e-02,  6.3446e-02,  2.5405e-02,\n",
       "                      -6.2758e-02, -6.7138e-02, -3.1350e-02, -6.3082e-02,  4.8295e-02,\n",
       "                       5.9586e-02,  3.6726e-02, -1.2140e-01,  9.4453e-03,  4.7052e-02,\n",
       "                       1.9261e-03,  6.5137e-03,  1.1772e-01, -3.5716e-02,  1.7685e-02,\n",
       "                       3.5142e-02, -3.3801e-02, -3.7003e-02,  5.3656e-02,  6.2224e-02,\n",
       "                       5.4207e-02, -6.4050e-02,  1.6294e-02,  1.7166e-02, -1.7704e-03,\n",
       "                       2.2700e-02, -4.3783e-02,  7.9215e-02,  6.1376e-02,  4.1986e-02,\n",
       "                       2.7342e-02, -6.2829e-02, -1.3026e-02,  6.1818e-03, -2.0240e-02])),\n",
       "             ('lstm.bias_hh_l2',\n",
       "              tensor([ 1.3044e-01, -1.8496e-02, -1.9330e-03, -4.0671e-02,  7.0977e-02,\n",
       "                      -3.3282e-02,  4.5025e-02, -4.7570e-02, -1.1211e-01,  2.1259e-01,\n",
       "                       2.4648e-01,  5.4034e-02, -5.1804e-02, -5.0687e-02,  1.5266e-01,\n",
       "                       4.3820e-03,  5.9808e-02,  1.0342e-01,  6.5167e-02,  2.2049e-02,\n",
       "                       2.1319e-01, -1.2248e-01, -9.6390e-02, -2.7184e-02, -2.7219e-04,\n",
       "                       1.4334e-01,  5.5723e-02,  1.3485e-01,  6.7064e-02,  1.1226e-02,\n",
       "                       2.6895e-01, -2.1717e-02,  3.9985e-02,  6.6987e-03,  5.4033e-02,\n",
       "                      -2.9855e-02,  1.9249e-01,  3.8303e-02,  9.8239e-02,  2.9341e-02,\n",
       "                       1.3530e-01,  1.5643e-02,  6.0319e-02,  1.0814e-01, -1.8865e-02,\n",
       "                       3.7085e-02,  7.4878e-02, -9.2914e-02, -5.8058e-03, -4.2986e-02,\n",
       "                       4.2407e-02,  4.7546e-02,  5.5716e-02, -1.6422e-02,  9.8998e-02,\n",
       "                      -1.7116e-02,  1.5306e-02,  1.2150e-03,  2.8876e-01,  1.2010e-01,\n",
       "                       3.5592e-03, -1.7827e-02,  3.5262e-02, -6.3541e-02,  4.7389e-01,\n",
       "                      -5.5871e-02, -3.9692e-02,  1.3179e-01,  3.1803e-01,  1.1099e-02,\n",
       "                      -9.1359e-02, -1.1413e-01,  5.9049e-02,  2.9843e-01,  8.7497e-02,\n",
       "                       2.5075e-01, -4.6737e-02, -1.2086e-01, -5.4874e-02, -9.3578e-03,\n",
       "                       4.7892e-02,  8.5807e-02,  1.1401e-01,  2.0119e-01,  1.8406e-01,\n",
       "                       6.1382e-02, -5.5228e-02,  7.7151e-02, -2.5287e-02, -1.8197e-01,\n",
       "                       4.4563e-02,  4.9704e-02, -4.8248e-02,  9.0456e-03,  3.7209e-02,\n",
       "                      -7.6745e-02,  9.2823e-02,  4.7205e-02,  4.1479e-02,  7.6726e-02,\n",
       "                      -5.8987e-02, -3.5091e-02,  1.1013e-02,  9.7036e-02,  3.2195e-02,\n",
       "                       1.1959e-01, -5.2157e-02,  1.5707e-01,  1.0835e-01, -1.1341e-01,\n",
       "                       9.1897e-02, -1.4753e-02,  5.8327e-02,  8.2631e-02,  5.8299e-02,\n",
       "                      -2.0278e-02,  5.8840e-02,  4.6270e-02, -1.0213e-01, -8.8473e-02,\n",
       "                       1.2511e-01,  1.5522e-01, -8.1004e-02,  3.7374e-02,  1.8203e-02,\n",
       "                       9.4087e-02,  1.5824e-02,  7.6723e-02,  2.0010e-02, -8.5418e-02,\n",
       "                       6.3383e-02, -4.3860e-02, -4.5194e-02, -2.8778e-02, -9.9716e-02,\n",
       "                      -9.7770e-02, -1.1852e-01, -1.2879e-01,  2.1065e-02, -3.5923e-03,\n",
       "                      -7.1206e-02,  6.4998e-02, -1.1433e-01, -1.5483e-02, -8.8625e-02,\n",
       "                      -4.0335e-02,  1.8196e-02, -8.5118e-02, -4.5543e-02,  3.0733e-02,\n",
       "                       9.0668e-02, -1.1797e-01, -2.4680e-02,  1.0802e-03, -1.1358e-02,\n",
       "                      -9.1288e-02, -6.5908e-02,  5.9641e-02,  6.2998e-02, -2.4276e-02,\n",
       "                      -1.7605e-02,  2.7572e-02, -1.0349e-02,  7.7459e-03,  3.5095e-02,\n",
       "                      -1.0402e-01,  2.5017e-03, -1.2857e-02,  8.5411e-04,  1.3813e-02,\n",
       "                       5.3700e-02,  1.6417e-03,  5.1608e-02,  8.8634e-02,  5.1140e-03,\n",
       "                       6.2335e-02, -1.1364e-01,  2.1441e-02, -1.0501e-03, -3.1449e-02,\n",
       "                      -5.9731e-02,  2.2843e-02, -7.5634e-02,  5.4556e-02, -1.1487e-02,\n",
       "                       5.1173e-02,  6.6180e-02, -1.1907e-02,  1.2245e-01,  2.5762e-02,\n",
       "                      -7.4404e-02,  3.5062e-02, -1.2549e-01, -2.6844e-02, -1.7258e-02,\n",
       "                       1.4194e-02,  9.8036e-02,  3.7844e-02, -8.9210e-02, -6.8418e-02,\n",
       "                      -1.9273e-02,  6.2274e-03,  1.5783e-02,  2.3745e-02, -8.9042e-02,\n",
       "                       3.2848e-03,  7.5380e-02, -8.2344e-02, -1.0678e-02,  3.7592e-02,\n",
       "                       2.8080e-02,  3.5794e-02, -4.3470e-02,  3.1636e-02, -2.6832e-02,\n",
       "                       8.8974e-04,  7.4405e-02,  3.8645e-02,  8.7417e-02, -3.8237e-03,\n",
       "                      -1.3203e-02, -1.9465e-02, -5.8072e-02,  6.2504e-02, -2.0064e-02,\n",
       "                       6.2068e-02,  5.4042e-02, -3.1189e-02, -2.5218e-02,  3.2552e-03,\n",
       "                      -3.5861e-02,  2.6817e-02, -3.5951e-02, -4.7558e-02,  4.6362e-02,\n",
       "                       1.4063e-02, -1.9932e-02, -5.8919e-02, -5.2127e-02,  8.4087e-03,\n",
       "                       1.9957e-02, -3.1349e-03,  4.9067e-02,  8.4472e-03,  6.9549e-03,\n",
       "                      -1.3494e-02,  2.7083e-02, -2.7017e-02, -8.2672e-03,  6.4321e-02,\n",
       "                      -3.0441e-02, -1.8334e-02, -6.0405e-02, -9.0362e-02,  9.6972e-03,\n",
       "                      -2.8580e-03, -2.0528e-02, -5.8011e-02, -1.9007e-02,  1.8190e-02,\n",
       "                       9.7585e-02,  9.4352e-03,  1.1196e-01,  2.2566e-02,  4.9384e-02,\n",
       "                       8.2271e-03,  6.0850e-02, -9.4748e-03, -1.6461e-02,  2.7106e-02,\n",
       "                      -7.0793e-02,  4.3915e-02,  9.4988e-03, -6.8831e-02, -4.1727e-02,\n",
       "                      -1.0589e-01, -9.3011e-02, -1.6439e-01,  1.1470e-01, -5.0486e-02,\n",
       "                       1.1608e-01, -4.2533e-02, -6.1460e-02, -4.3399e-02, -1.2682e-01,\n",
       "                       1.1896e-01,  1.2443e-01, -1.1428e-01,  2.6135e-02,  2.8905e-02,\n",
       "                       3.0127e-01, -8.6556e-02, -4.8896e-03, -1.6327e-01,  4.7304e-02,\n",
       "                       4.7961e-02, -2.0913e-01,  1.2272e-01, -1.1602e-01, -9.9344e-03,\n",
       "                      -1.0203e-01, -1.7907e-02,  7.7909e-02,  1.8119e-01,  4.5629e-03,\n",
       "                       1.7769e-01, -8.9046e-02, -3.5095e-02,  6.4941e-02,  2.3627e-03,\n",
       "                      -1.0305e-01, -1.0017e-02, -1.5642e-02, -2.7532e-02,  2.1842e-01,\n",
       "                      -8.8663e-02, -2.9697e-02,  1.1427e-01, -1.3752e-01, -6.3994e-02,\n",
       "                       1.2445e-02, -5.9994e-02,  6.8612e-02,  1.9662e-01, -2.2732e-01,\n",
       "                      -8.3454e-02,  1.4077e-01,  5.7019e-02,  9.4744e-02, -1.4192e-01,\n",
       "                       6.1177e-02, -5.3289e-02,  1.5290e-01,  3.0815e-01,  8.6106e-02,\n",
       "                      -2.8596e-02,  7.2292e-02,  1.5947e-02, -1.5621e-02, -1.1446e-01,\n",
       "                       7.3184e-02, -4.4778e-02,  4.9482e-03, -3.0814e-02,  5.1682e-02,\n",
       "                       6.5376e-02,  3.8624e-02,  1.2782e-02, -8.7908e-02, -8.0638e-02,\n",
       "                       3.3362e-02, -1.6220e-02, -2.4321e-02, -1.0029e-01, -1.1245e-01,\n",
       "                      -6.5624e-02,  1.5699e-01, -5.5408e-02, -6.7588e-02, -4.5898e-02,\n",
       "                      -3.8712e-02,  7.7421e-02, -1.7619e-01, -1.5148e-02, -1.6050e-02,\n",
       "                      -4.5229e-02,  5.6273e-02, -8.1204e-02,  8.8345e-02,  1.2132e-01,\n",
       "                      -9.3586e-02,  3.3739e-02,  3.5136e-02,  4.6592e-02,  2.2972e-02,\n",
       "                      -1.1461e-01, -1.0130e-01, -1.5046e-01, -5.4900e-02, -3.1505e-02,\n",
       "                      -2.8970e-02,  9.0174e-02, -1.1124e-01, -4.8691e-02, -6.7609e-02,\n",
       "                       5.8313e-02, -5.2753e-02,  1.4421e-01, -2.5859e-01, -2.4879e-02,\n",
       "                       1.5046e-02, -8.4031e-02,  6.4026e-02,  2.9726e-02,  3.4667e-02,\n",
       "                      -6.0075e-02,  9.0652e-03, -4.2276e-02, -1.4265e-02,  1.3791e-01,\n",
       "                       1.5353e-02, -4.7248e-03, -5.4649e-03, -3.8154e-02,  4.9185e-02,\n",
       "                      -4.3228e-02, -9.7906e-02,  5.0475e-02, -6.6098e-02, -1.6453e-02,\n",
       "                       7.0262e-02, -3.2286e-02, -2.5361e-03,  2.4241e-02, -7.1791e-03,\n",
       "                      -4.9839e-02,  1.8925e-01, -6.6639e-02, -5.8187e-02, -5.8348e-02,\n",
       "                       1.3882e-01,  4.6802e-02, -3.2145e-02,  2.0395e-02,  2.7131e-02,\n",
       "                      -1.9580e-02,  4.4472e-02,  1.0517e-03, -1.7663e-02,  4.3080e-02,\n",
       "                      -7.9676e-02,  5.2298e-02,  3.8005e-02, -5.7290e-02,  3.7714e-02,\n",
       "                      -6.4256e-02,  3.0105e-02, -4.7909e-02, -3.9457e-02, -6.2348e-02,\n",
       "                       9.7659e-02,  2.8689e-02, -9.6664e-02,  7.1791e-02, -4.0402e-02,\n",
       "                      -2.2410e-04,  7.3110e-02,  7.2063e-02,  2.6777e-01,  1.7633e-01,\n",
       "                       8.4074e-02,  6.6582e-02,  1.5314e-01,  7.2699e-02, -2.4815e-03,\n",
       "                      -7.3296e-02,  4.6966e-02, -4.9131e-02, -5.1312e-02, -2.3468e-02,\n",
       "                      -8.4772e-02,  8.8022e-03, -8.1295e-02,  1.2774e-01,  5.6372e-02,\n",
       "                       1.7393e-01, -3.2792e-02, -7.8369e-02,  8.8919e-02, -7.1701e-03,\n",
       "                       1.8425e-02,  2.3142e-02, -6.7069e-02,  5.2839e-02,  2.5118e-02,\n",
       "                      -1.6206e-02, -8.2637e-03,  1.7406e-02,  4.6331e-02, -5.3326e-02,\n",
       "                      -7.0065e-02, -9.2022e-02, -6.2665e-02,  4.0952e-03,  6.2433e-02,\n",
       "                       2.2486e-02,  5.1583e-02,  3.1859e-02,  9.3470e-03, -1.2907e-02,\n",
       "                       1.8179e-02, -5.2658e-02, -1.1296e-01, -7.9985e-02,  3.8019e-02,\n",
       "                      -1.5243e-02, -9.2535e-03,  1.1554e-01, -2.4489e-02, -7.3108e-02,\n",
       "                       4.8171e-02,  3.5050e-02, -4.4640e-03,  2.8450e-02,  1.2126e-01,\n",
       "                      -7.8129e-03, -4.9728e-02, -6.6821e-02,  3.7360e-02, -4.5361e-02,\n",
       "                      -8.0742e-03,  8.2542e-02,  2.7471e-02,  9.9872e-02,  2.9690e-02,\n",
       "                      -2.4142e-02, -4.8908e-02, -5.9878e-02,  3.9643e-02, -5.2259e-02])),\n",
       "             ('fully_connected.0.weight',\n",
       "              tensor([[-0.0215, -0.0838, -0.3083,  ...,  0.2104,  0.0273, -0.0559],\n",
       "                      [-0.0615, -0.0451, -0.1032,  ..., -0.0696, -0.3839,  0.0040],\n",
       "                      [ 0.1148, -0.2513,  0.0428,  ..., -0.0443, -0.1876,  0.3050],\n",
       "                      ...,\n",
       "                      [-0.3526, -0.2518,  0.1648,  ..., -0.8274, -0.3918, -0.4776],\n",
       "                      [-0.0828,  0.0371,  0.0072,  ..., -0.0170, -0.0314,  0.0173],\n",
       "                      [-0.1110,  0.0549, -0.1189,  ...,  0.0217, -0.2734, -0.0792]])),\n",
       "             ('fully_connected.0.bias',\n",
       "              tensor([ 0.0405,  0.0526,  0.0064, -0.0536, -0.0293, -0.0763,  0.0572,  0.0873,\n",
       "                       0.0609, -0.0427,  0.0466, -0.0199, -0.0368, -0.0101,  0.0856,  0.0506,\n",
       "                      -0.0292, -0.0999,  0.1519,  0.3149,  0.0698,  0.0465,  0.0322,  0.0111,\n",
       "                       0.0699,  0.3381,  0.0650,  0.3325,  0.0310, -0.0542, -0.0656,  0.0157,\n",
       "                       0.0615,  0.0036,  0.0335,  0.0278,  0.3000,  0.0262,  0.0719, -0.0244,\n",
       "                      -0.0962,  0.0604,  0.3679, -0.0779,  0.0589,  0.1547, -0.0400, -0.0152,\n",
       "                      -0.0079, -0.0473,  0.0846,  0.0409,  0.0325,  0.0726,  0.0138, -0.0690,\n",
       "                      -0.0916, -0.0530,  0.0352, -0.0538,  0.0029,  0.0995, -0.0233,  0.0148,\n",
       "                       0.3151,  0.0179,  0.0037,  0.0263,  0.3019, -0.0398,  0.0767,  0.0933,\n",
       "                       0.2694,  0.2725,  0.0799, -0.0732, -0.0213, -0.1054,  0.0201,  0.0349,\n",
       "                      -0.0539,  0.0309,  0.0268, -0.0677,  0.0886, -0.0737, -0.0433,  0.4140,\n",
       "                       0.0645,  0.0087, -0.0372,  0.0673,  0.0333,  0.0802,  0.0126,  0.0145,\n",
       "                      -0.0228, -0.0589, -0.0129,  0.0044, -0.0388,  0.0050, -0.0444,  0.0851,\n",
       "                      -0.1262,  0.0421, -0.0408, -0.0438, -0.0599,  0.3391,  0.0700,  0.0046,\n",
       "                      -0.0737, -0.0336,  0.1177,  0.1664, -0.0778,  0.1439,  0.0549, -0.0722,\n",
       "                       0.0908,  0.1966, -0.0813, -0.0660,  0.0563,  0.1403,  0.0323,  0.3518,\n",
       "                       0.0436, -0.0436])),\n",
       "             ('fully_connected.2.weight',\n",
       "              tensor([[-0.0262,  0.1985,  0.0774,  ...,  0.2927,  0.0545,  0.2571],\n",
       "                      [ 0.0854,  0.2135, -0.0625,  ..., -0.4521, -0.0418,  0.1760],\n",
       "                      [ 0.2650,  0.0820,  0.0430,  ..., -0.7888,  0.0448,  0.1185],\n",
       "                      ...,\n",
       "                      [-0.3566,  0.2843,  0.1268,  ..., -3.9611, -0.0308, -0.5297],\n",
       "                      [ 0.0350, -0.8108, -0.1754,  ..., -0.3468, -0.0103,  0.0790],\n",
       "                      [ 0.1681,  0.0890,  0.1118,  ..., -2.2497, -0.0111,  0.3717]])),\n",
       "             ('fully_connected.2.bias',\n",
       "              tensor([ 0.0782,  0.0189, -0.0827,  0.0604,  0.0244, -0.1878,  0.0079, -0.2106,\n",
       "                      -0.0235, -0.0738,  0.0704, -0.0605,  0.0771,  0.0238,  0.0745,  0.0583,\n",
       "                       0.0562, -0.0524, -0.0397, -0.0445, -0.1613,  0.0396, -0.1538,  0.0583,\n",
       "                       0.1442, -0.0377,  0.0987, -0.0191, -0.0535,  0.0141, -0.0517,  0.0847,\n",
       "                       0.0443,  0.0626, -0.1599,  0.0010,  0.0219, -0.1037,  0.0713,  0.1020,\n",
       "                      -0.0105,  0.0558,  0.1075,  0.0653, -0.0457, -0.1089, -0.0320, -0.0129,\n",
       "                      -0.0408,  0.0335,  0.1647, -0.1316,  0.0240,  0.0491,  0.0250, -0.1934,\n",
       "                       0.0200,  0.0198,  0.0136, -0.0872,  0.0361,  0.0996,  0.0225, -0.0676,\n",
       "                       0.0343]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_predictor.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_SAVE_PATH: models\\shakespeare_LSTM.pt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Create models directory \n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path \n",
    "MODEL_NAME = \"shakespeare_LSTM.pt\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Save the model state dict \n",
    "print(f\"MODEL_SAVE_PATH: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=shakespeare_predictor.state_dict(), # only saving the parameters\n",
    "           f=MODEL_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
