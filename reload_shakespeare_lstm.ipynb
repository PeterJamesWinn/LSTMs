{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class text:\n",
    "    def __init__(self, text_name, is_file):\n",
    "        \n",
    "        # Retrieve text\n",
    "        self.text = ''\n",
    "        self.encoded = ''\n",
    "        if is_file == True:\n",
    "            print(\"reading text from file\")\n",
    "            self.from_file(text_name)\n",
    "        else:\n",
    "            self.from_string(text_name)\n",
    "        \n",
    "        # Calculate vocab size, i.e. the number of characters; \n",
    "        # first get sorted list of unique characters\n",
    "        self.chars = sorted(list(set(self.text)))\n",
    "        #self.vocab_size = self.vocab_size(chars)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.str_to_int = {}\n",
    "        self.int_to_str = {}\n",
    "        self.str_to_int = self.make_str_to_int_table(self.chars)\n",
    "        self.int_to_str = self.make_int_to_str_table(self.chars)\n",
    "    \n",
    "    def from_file(self, filename):\n",
    "        'Read text from file and calculate vocab size'\n",
    "        self.text = open(filename,'r',encoding='utf-8').read()\n",
    "    \n",
    "    def from_string(self, string):\n",
    "        'Read text from string and calculate vocab size'\n",
    "        self.text = string\n",
    "    \n",
    "    #def calc_vocab_size(self, chars):\n",
    "    #    'Calculate vocab size'\n",
    "    #    self.vocab_size = len(chars)\n",
    "        \n",
    "    def make_str_to_int_table(self, chars):\n",
    "        '''Populate dictionary of character to integer mapping'''\n",
    "        return  {character: int for int, character in enumerate(chars)}\n",
    "    \n",
    "    def make_int_to_str_table(self, chars):\n",
    "        '''Populate dictionary of character to integer mapping'''\n",
    "        return  {int: character for int, character in enumerate(chars)}\n",
    "    \n",
    "    def encode_text_as_tensor(self):\n",
    "        '''encode the training text as a list of integers \n",
    "        and then convert to tensor with which to replace self.text'''\n",
    "        encode = lambda char:[self.str_to_int[char] for char in self.text]\n",
    "        self.text = torch.tensor(encode(self.text), dtype = torch.long)\n",
    "    \n",
    "    def encode_new_text_as_tensor(self, to_encode):\n",
    "        '''encode a new text as a list of integers, according to the \n",
    "        encoding derived from the training text. Return a tensor'''\n",
    "        encode = lambda char:[self.str_to_int[char] for char in to_encode]\n",
    "        return torch.tensor(encode(to_encode), dtype = torch.long)\n",
    "\n",
    "    def decode(self, to_decode):\n",
    "        '''decode from a list of integers to a string, using the\n",
    "        encoding vocabulary attached to the object: self.int_to_str '''  \n",
    "        decode = lambda l: ''.join([self.int_to_str[i] for i in l])\n",
    "        return decode(to_decode)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.vocab_size}\"\n",
    "    \n",
    "    def make_train_val_test(self, fraction_train, fraction_val, fraction_test):\n",
    "        '''simple train /validation sets.  no randomisation \n",
    "         of selections, so assuming  no bias in the distribution within the data file'''\n",
    "        if fraction_train + fraction_test + fraction_val != 1:\n",
    "            print(\"Warning, fractions of train, test and validation do \\\n",
    "                  not add to one.\")\n",
    "        n = int(fraction_train*len(self.text))\n",
    "        nv = int(fraction_val*len(self.text))\n",
    "        nt = int(fraction_test*len(self.text))\n",
    "        self.train_data = self.text[:n]\n",
    "        self.val_data = self.text[n:n+nv]\n",
    "        self.test_data = self.text[n+nv:n+nv+nt]\n",
    "    \n",
    "    def get_batch(self, batch_size, block_size, train_test_validation):\n",
    "        \"\"\"Randomly pick data from the training data/test data/validation\n",
    "        and return as a batch stacked in a torch tensor.\"\"\"\n",
    "        if train_test_validation == \"train\":\n",
    "            data = self.train_data\n",
    "        elif train_test_validation == \"test\":\n",
    "            data = self.test_data\n",
    "        elif train_test_validation == \"validation\":\n",
    "            data = self.val_data\n",
    "        else:\n",
    "            raise \\\n",
    "            ValueError(\"Specify data set as 'train', 'test', or 'validation'.\")\n",
    "\n",
    "        if len(data) < block_size:\n",
    "            raise ValueError(\"Data is smaller than the specified block size.\")\n",
    "\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        train_context_batch = torch.stack([data[i:i + block_size] for i in ix])\n",
    "        train_to_predict_batch =\\\n",
    "              torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "        return train_context_batch, train_to_predict_batch\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(number_eval_batches, \n",
    "                  language_model, training_text_object, block_size):\n",
    "  '''Take number_eval_batches from training and validation sets and \n",
    "  calculate an average loss for each. Return a dictionary with the\n",
    "  two losses, with keys train and validation '''\n",
    "  out = {}\n",
    "  language_model.eval()\n",
    "  for split in ['train', 'validation']:\n",
    "    losses = torch.zeros(number_eval_batches)\n",
    "    for k in range(number_eval_batches):\n",
    "      X, Y = training_text_object.get_batch(1, block_size, split)\n",
    "      logits, loss = language_model(X, Y)\n",
    "      losses[k] = loss.item()\n",
    "    out[split] = losses.mean()\n",
    "  language_model.train()\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM  \n",
    "#\n",
    "class LanguageModel_LSTM(nn.Module):\n",
    "  '''logits produced from an LSTM over the context (length block_size) \n",
    "   of the previous characters, \n",
    "    from which we are trying to predict the current character; \n",
    "  generate() uses logits as a multivariate distribution from which \n",
    "    to predict next character. \n",
    "  \n",
    "   Training learns the values in the embedding vector, the weights of \n",
    "    RNN and the weights of the fully connected layer prior to the\n",
    "     softmax '''\n",
    "\n",
    "  def __init__(self, vocab_size, hidden_size, num_layers, verbose=False):\n",
    "    super().__init__()\n",
    "    self.verbose=verbose\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    # make all embeddings small so the logits produced give a\n",
    "    # probability distribution is equal in all direction.\n",
    "    # maximum initial entropy should give the most likely low cross entropy, \n",
    "    # since initial guess is not likely to be better than equal uncertainty.\n",
    "    with torch.no_grad():\n",
    "      self.token_embedding_table.weight.data \\\n",
    "      = self.token_embedding_table.weight.data * 0.01\n",
    "    self.input_size = vocab_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers,\\\n",
    "                       batch_first=True) # (h0, c0) default to zero.\n",
    "    self.fully_connected = nn.Sequential(\n",
    "                       nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(self.hidden_size, vocab_size)\n",
    "                       )\n",
    "\n",
    "    \n",
    "  def set_verbose(self, verbose):\n",
    "    self.verbose = verbose\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # idx - (index of) the x values, i.e. the context vector\n",
    "    # idx and targets are both (B, T) tensor of integers (see comment below)\n",
    "    #h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "    # default h0 for RNN model  is to set everything to zeros\n",
    "    logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "    # nn.RNN produces batch_size, sequ, hidden_size,\n",
    "    out, _ = self.lstm(logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #logits = self.fully_connected(logits)\n",
    "    logits = self.fully_connected(out)\n",
    "    # B T C is batch by time by channel\n",
    "    # batch  is number of the batch\n",
    "    # time is block size\n",
    "    # channel is the vocab size \n",
    "    if targets == None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape # need to reshape to be correct shape for cross_entropy\n",
    "      # use view in pytorch that changes the view of the data passed but not the\n",
    "      # underlying data. Flatten the first two dimensions of the logits, \n",
    "      # to create\n",
    "      # a batch of size B*T with the channel data,\n",
    "      # i.e. vocab or probability of each character/class as the second\n",
    "      # dimension, which is what F.cross_entropy expects.\n",
    "      # Similarly, targets reduced to 1 dimension of batch data (B*T) \n",
    "      # with class lable in each dimension. \n",
    "      if self.verbose: print(f\"Targets: \\n {targets} \\n; logits: \\n{logits} \")\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      if self.verbose: print(f\"Transformed view: Targets: \\n {targets} \\n; logits:\\n{logits}\\n\")\n",
    "      # cross_entropy includes a softmax transformation. \n",
    "      if self.verbose:\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        idx_next = torch.multinomial(probs, num_samples = 1) \n",
    "        print(f\"Generative outcomes during training:\\n {idx_next}\")\n",
    "\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    '''Predict the next character/token from the learnt distribution,adding\n",
    "    it to the current context, idx, till max_new_tokens have been added.'''\n",
    "    for _ in range(max_new_tokens):\n",
    "      #get the predictions\n",
    "      logits, _ = self(idx) # calls forwards() for this class\n",
    "      \n",
    "      # Following line: \n",
    "      # h_out from the last character of each sequence in the batch.\n",
    "      logits = logits[:, -1, :] \n",
    "      # logits now has dimension B, C, since only one token.\n",
    "      \n",
    "      # Generate probability distribution\n",
    "      probs = F.softmax(logits, dim=-1) # B, C\n",
    "      if self.verbose:\n",
    "          print(f'logits {logits}, {logits.shape} \\n probs {probs}, {probs.shape}')\n",
    "      \n",
    "      # Sample  vocabulary according to the probability distribution, probs.\n",
    "      idx_next = torch.multinomial(probs, num_samples = 1) # B by 1 size\n",
    "      # Append sample to sequence\n",
    "      idx = torch.cat((idx, idx_next), dim = 1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading text from file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Create models directory \n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path \n",
    "MODEL_NAME = \"shakespeare_LSTM.pt\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# loading saved models.\n",
    "# Instantiate a fresh instance of model\n",
    "shakespeare = text(\"input.txt\", is_file=True)\n",
    "shakespeare.encode_text_as_tensor()\n",
    "vocab_size = 65\n",
    "hidden_size = shakespeare.vocab_size*2\n",
    "input_size = shakespeare.vocab_size\n",
    "num_layers = 3\n",
    "shakespeare_LSTM = LanguageModel_LSTM(input_size, \\\n",
    "                                hidden_size, num_layers, verbose=False)\n",
    "\n",
    "\n",
    "# Load model state dict \n",
    "shakespeare_LSTM.load_state_dict(torch.load(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([[1]])\n",
      "tensor([[ 1, 50, 53, 52, 45,  1, 58, 46, 39, 58,  1, 61, 47, 57, 46,  0, 32, 46,\n",
      "         39, 58,  1, 40, 43, 43, 52,  6,  1, 21,  1, 46, 39, 42,  0, 47, 57,  1,\n",
      "         53,  5,  1, 46, 43, 10,  1, 47, 44,  1, 58, 46, 43, 43,  1, 58, 46, 53,\n",
      "         59,  1, 39, 52, 42,  1, 57, 53,  1, 57, 39, 51, 43,  1, 39, 50, 50,  1,\n",
      "         58, 53,  1, 57, 53, 51, 43,  6,  0, 25, 39, 49, 43, 57,  1, 39,  1, 57,\n",
      "         58, 61, 39, 52, 42,  1, 58, 46, 43, 51,  1, 39, 52, 42,  1, 46, 47, 57,\n",
      "          1, 41, 53, 59, 56, 57, 43,  7, 46, 53, 59, 57, 43,  6,  0, 13, 52, 42,\n",
      "          1, 61, 43,  1, 46, 53, 56, 52,  6,  1, 39,  1, 51, 47, 52, 63,  1, 53,\n",
      "         54, 43, 52,  1, 47, 52, 55, 59, 47, 56, 43, 42,  0, 32, 53,  1, 46, 47,\n",
      "         57,  1, 58, 53, 45, 43, 58, 46, 43, 56,  1, 46, 47, 57,  1, 44, 43, 50,\n",
      "         50,  5, 42,  1, 58, 46, 43,  1, 28, 56, 53, 42, 47, 58,  1, 39, 41, 41,\n",
      "         53, 59, 52, 58,  1, 57, 39, 47, 42,  8,  0, 19, 53,  1, 51, 43,  1, 21,\n",
      "          1, 57, 43, 43,  6,  1, 21,  1, 40, 63,  1, 44, 53, 56, 61, 53, 56, 52,\n",
      "          1, 44, 39, 50, 57, 43,  8,  0,  0, 25, 17, 30, 15, 33, 32, 21, 27, 10,\n",
      "          0, 35, 46, 43, 52,  6,  1, 51, 63,  1, 42, 53, 41, 58, 39, 47, 52, 57,\n",
      "          6,  1, 40, 63,  1, 57, 50, 39, 47, 52,  8,  0,  0, 15, 27, 30, 21, 27,\n",
      "         24, 13, 26, 33, 31, 10,  0, 24, 43, 58,  1, 51, 43]])\n",
      " long that wish\n",
      "That been, I had\n",
      "is o' he: if thee thou and so same all to some,\n",
      "Makes a stwand them and his course-house,\n",
      "And we horn, a miny open inquired\n",
      "To his together his fell'd the Prodit account said.\n",
      "Go me I see, I by forworn false.\n",
      "\n",
      "MERCUTIO:\n",
      "When, my doctains, by slain.\n",
      "\n",
      "CORIOLANUS:\n",
      "Let me\n"
     ]
    }
   ],
   "source": [
    "# current model seems to be still decreasing in loss, albeit very slowly\n",
    "context = ' '\n",
    "context = shakespeare.encode_new_text_as_tensor(context)\n",
    "print(context)\n",
    "context = torch.unsqueeze(context, dim=0)\n",
    "print(context)\n",
    "shakespeare_LSTM.set_verbose(False)\n",
    "new_string = shakespeare_LSTM.generate(context,300)\n",
    "print(new_string)\n",
    "print(shakespeare.decode(new_string[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
