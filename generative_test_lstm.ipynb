{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Herein are cells that \n",
    "# 1. Create a text class that has methods to determine the vocabulary\n",
    "# within the text, encode/decode the text, split the text content into\n",
    "# validation, training and test sets.\n",
    "# \n",
    "# 2. LSTM language model for training and generating text.\n",
    "#\n",
    "# 3. Reading in of a simple test case\n",
    "#  Training and validation loop for simple test case.\n",
    "#\n",
    "# 4. Tiny Shakespeare training and generation. \n",
    "# \n",
    "\n",
    "# Refactoring - there is a logic in moving the vocab/encoding/decoding \n",
    "# methods from the text class into the language model, since the language\n",
    "# model has to deal with with an appropriately encoded/decoded \n",
    "# set of data - so saving the weights of the language model is useless\n",
    "# without knowing the encodings! On the otherhand ... that's to \n",
    "# put two different types of things in the same object ... so leave as\n",
    "# is for the moment. \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class text:\n",
    "    def __init__(self, text_name, is_file):\n",
    "        \n",
    "        # Retrieve text\n",
    "        self.text = ''\n",
    "        self.encoded = ''\n",
    "        if is_file == True:\n",
    "            print(\"reading text from file\")\n",
    "            self.from_file(text_name)\n",
    "        else:\n",
    "            self.from_string(text_name)\n",
    "        \n",
    "        # Calculate vocab size, i.e. the number of characters; \n",
    "        # first get sorted list of unique characters\n",
    "        self.chars = sorted(list(set(self.text)))\n",
    "        #self.vocab_size = self.vocab_size(chars)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.str_to_int = {}\n",
    "        self.int_to_str = {}\n",
    "        self.str_to_int = self.make_str_to_int_table(self.chars)\n",
    "        self.int_to_str = self.make_int_to_str_table(self.chars)\n",
    "    \n",
    "    def from_file(self, filename):\n",
    "        'Read text from file and calculate vocab size'\n",
    "        self.text = open(filename,'r',encoding='utf-8').read()\n",
    "    \n",
    "    def from_string(self, string):\n",
    "        'Read text from string and calculate vocab size'\n",
    "        self.text = string\n",
    "    \n",
    "    #def calc_vocab_size(self, chars):\n",
    "    #    'Calculate vocab size'\n",
    "    #    self.vocab_size = len(chars)\n",
    "        \n",
    "    def make_str_to_int_table(self, chars):\n",
    "        '''Populate dictionary of character to integer mapping'''\n",
    "        return  {character: int for int, character in enumerate(chars)}\n",
    "    \n",
    "    def make_int_to_str_table(self, chars):\n",
    "        '''Populate dictionary of character to integer mapping'''\n",
    "        return  {int: character for int, character in enumerate(chars)}\n",
    "    \n",
    "    def encode_text_as_tensor(self):\n",
    "        '''encode the training text as a list of integers \n",
    "        and then convert to tensor with which to replace self.text'''\n",
    "        encode = lambda char:[self.str_to_int[char] for char in self.text]\n",
    "        self.text = torch.tensor(encode(self.text), dtype = torch.long)\n",
    "    \n",
    "    def encode_new_text_as_tensor(self, to_encode):\n",
    "        '''encode a new text as a list of integers, according to the \n",
    "        encoding derived from the training text. Return a tensor'''\n",
    "        encode = lambda char:[self.str_to_int[char] for char in to_encode]\n",
    "        return torch.tensor(encode(to_encode), dtype = torch.long)\n",
    "\n",
    "    def decode(self, to_decode):\n",
    "        '''decode from a list of integers to a string, using the\n",
    "        encoding vocabulary attached to the object: self.int_to_str '''  \n",
    "        decode = lambda l: ''.join([self.int_to_str[i] for i in l])\n",
    "        return decode(to_decode)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.vocab_size}\"\n",
    "    \n",
    "    def make_train_val_test(self, fraction_train, fraction_val, fraction_test):\n",
    "        '''simple train /validation sets.  no randomisation \n",
    "         of selections, so assuming  no bias in the distribution within the data file'''\n",
    "        if fraction_train + fraction_test + fraction_val != 1:\n",
    "            print(\"Warning, fractions of train, test and validation do \\\n",
    "                  not add to one.\")\n",
    "        n = int(fraction_train*len(self.text))\n",
    "        nv = int(fraction_val*len(self.text))\n",
    "        nt = int(fraction_test*len(self.text))\n",
    "        self.train_data = self.text[:n]\n",
    "        self.val_data = self.text[n:n+nv]\n",
    "        self.test_data = self.text[n+nv:n+nv+nt]\n",
    "    \n",
    "    def get_batch(self, batch_size, block_size, train_test_validation):\n",
    "        \"\"\"Randomly pick data from the training data/test data/validation\n",
    "        and return as a batch stacked in a torch tensor.\"\"\"\n",
    "        if train_test_validation == \"train\":\n",
    "            data = self.train_data\n",
    "        elif train_test_validation == \"test\":\n",
    "            data = self.test_data\n",
    "        elif train_test_validation == \"validation\":\n",
    "            data = self.val_data\n",
    "        else:\n",
    "            raise \\\n",
    "            ValueError(\"Specify data set as 'train', 'test', or 'validation'.\")\n",
    "\n",
    "        if len(data) < block_size:\n",
    "            raise ValueError(\"Data is smaller than the specified block size.\")\n",
    "\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        train_context_batch = torch.stack([data[i:i + block_size] for i in ix])\n",
    "        train_to_predict_batch =\\\n",
    "              torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "        return train_context_batch, train_to_predict_batch\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(number_eval_batches, \n",
    "                  language_model, training_text_object, block_size):\n",
    "  '''Take number_eval_batches from training and validation sets and \n",
    "  calculate an average loss for each. Return a dictionary with the\n",
    "  two losses, with keys train and validation '''\n",
    "  out = {}\n",
    "  language_model.eval()\n",
    "  for split in ['train', 'validation']:\n",
    "    losses = torch.zeros(number_eval_batches)\n",
    "    for k in range(number_eval_batches):\n",
    "      X, Y = training_text_object.get_batch(1, block_size, split)\n",
    "      logits, loss = language_model(X, Y)\n",
    "      losses[k] = loss.item()\n",
    "    out[split] = losses.mean()\n",
    "  language_model.train()\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM  \n",
    "#\n",
    "class LanguageModel_LSTM(nn.Module):\n",
    "  '''logits produced from an LSTM over the context (length block_size) \n",
    "   of the previous characters, \n",
    "    from which we are trying to predict the current character; \n",
    "  generate() uses logits as a multivariate distribution from which \n",
    "    to predict next character. \n",
    "  \n",
    "   Training learns the values in the embedding vector, the weights of \n",
    "    RNN and the weights of the fully connected layer prior to the\n",
    "     softmax '''\n",
    "\n",
    "  def __init__(self, vocab_size, hidden_size, num_layers, verbose=False):\n",
    "    super().__init__()\n",
    "    self.verbose=verbose\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    # make all embeddings small so the logits produced give a\n",
    "    # probability distribution is equal in all direction.\n",
    "    # maximum initial entropy should give the most likely low cross entropy, \n",
    "    # since initial guess is not likely to be better than equal uncertainty.\n",
    "    with torch.no_grad():\n",
    "      self.token_embedding_table.weight.data \\\n",
    "      = self.token_embedding_table.weight.data * 0.01\n",
    "    self.input_size = vocab_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers,\\\n",
    "                       batch_first=True) # (h0, c0) default to zero.\n",
    "    self.fully_connected = nn.Sequential(\n",
    "                       nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(self.hidden_size, vocab_size)\n",
    "                       )\n",
    "\n",
    "    \n",
    "  def set_verbose(self, verbose):\n",
    "    self.verbose = verbose\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # idx - (index of) the x values, i.e. the context vector\n",
    "    # idx and targets are both (B, T) tensor of integers (see comment below)\n",
    "    #h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "    # default h0 for RNN model  is to set everything to zeros\n",
    "    logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "    # nn.RNN produces batch_size, sequ, hidden_size,\n",
    "    out, _ = self.lstm(logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #logits = self.fully_connected(logits)\n",
    "    logits = self.fully_connected(out)\n",
    "    # B T C is batch by time by channel\n",
    "    # batch  is number of the batch\n",
    "    # time is block size\n",
    "    # channel is the vocab size \n",
    "    if targets == None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape # need to reshape to be correct shape for cross_entropy\n",
    "      # use view in pytorch that changes the view of the data passed but not the\n",
    "      # underlying data. Flatten the first two dimensions of the logits, \n",
    "      # to create\n",
    "      # a batch of size B*T with the channel data,\n",
    "      # i.e. vocab or probability of each character/class as the second\n",
    "      # dimension, which is what F.cross_entropy expects.\n",
    "      # Similarly, targets reduced to 1 dimension of batch data (B*T) \n",
    "      # with class lable in each dimension. \n",
    "      if self.verbose: print(f\"Targets: \\n {targets} \\n; logits: \\n{logits} \")\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      if self.verbose: print(f\"Transformed view: Targets: \\n {targets} \\n; logits:\\n{logits}\\n\")\n",
    "      # cross_entropy includes a softmax transformation. \n",
    "      if self.verbose:\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        idx_next = torch.multinomial(probs, num_samples = 1) \n",
    "        print(f\"Generative outcomes during training:\\n {idx_next}\")\n",
    "\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    '''Predict the next character/token from the learnt distribution,adding\n",
    "    it to the current context, idx, till max_new_tokens have been added.'''\n",
    "    for _ in range(max_new_tokens):\n",
    "      #get the predictions\n",
    "      logits, _ = self(idx) # calls forwards() for this class\n",
    "      \n",
    "      # Following line: \n",
    "      # h_out from the last character of each sequence in the batch.\n",
    "      logits = logits[:, -1, :] \n",
    "      # logits now has dimension B, C, since only one token.\n",
    "      \n",
    "      # Generate probability distribution\n",
    "      probs = F.softmax(logits, dim=-1) # B, C\n",
    "      if self.verbose:\n",
    "          print(f'logits {logits}, {logits.shape} \\n probs {probs}, {probs.shape}')\n",
    "      \n",
    "      # Sample  vocabulary according to the probability distribution, probs.\n",
    "      idx_next = torch.multinomial(probs, num_samples = 1) # B by 1 size\n",
    "      # Append sample to sequence\n",
    "      idx = torch.cat((idx, idx_next), dim = 1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  ..., 25, 26,  0])\n",
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\customer\\AppData\\Local\\Temp\\ipykernel_2236\\194577180.py:73: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  self.text = torch.tensor(encode(self.text), dtype = torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Train language model on \"ABCD EFGH IJKL MNOP QRSTU VWXYZ \" string.\n",
    "# Define string\n",
    "# Read string into object\n",
    "# Encode string and place in a tensor\n",
    "string = \"ABCD EFGH IJKL MNOP QRSTU VWXYZ \"\n",
    "string = string * 100\n",
    "AB_string_3 = text(string, is_file=False)\n",
    "AB_string_3.encode_text_as_tensor()\n",
    "AB_string_3.make_train_val_test(0.8, 0.1, 0.1)\n",
    "print(AB_string_3.train_data)\n",
    "print(AB_string_3.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current iteration loss, at step 0: 3.2831077575683594\n",
      "step 0: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 3.3302,           \n",
      " validation loss 3.3374\n",
      "\n",
      "\n",
      "current iteration loss, at step 500: 0.893225908279419\n",
      "step 500: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 1.0081,           \n",
      " validation loss 1.1495\n",
      "\n",
      "\n",
      "current iteration loss, at step 1000: 0.36434099078178406\n",
      "step 1000: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.1959,           \n",
      " validation loss 0.2853\n",
      "\n",
      "\n",
      "current iteration loss, at step 1500: 0.036115728318691254\n",
      "step 1500: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.2230,           \n",
      " validation loss 0.0806\n",
      "\n",
      "\n",
      "current iteration loss, at step 2000: 0.24428200721740723\n",
      "step 2000: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.0996,           \n",
      " validation loss 0.0669\n",
      "\n",
      "\n",
      "current iteration loss, at step 2500: 0.010101783089339733\n",
      "step 2500: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.0861,           \n",
      " validation loss 0.1405\n",
      "\n",
      "\n",
      "current iteration loss, at step 3000: 0.0036862387787550688\n",
      "step 3000: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.2536,           \n",
      " validation loss 0.0591\n",
      "\n",
      "\n",
      "current iteration loss, at step 3500: 0.0032198650296777487\n",
      "step 3500: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.1548,           \n",
      " validation loss 0.0970\n",
      "\n",
      "\n",
      "final loss: \n",
      "         loss averaged over 10 batches:        \n",
      " training loss: 0.0452,         \n",
      " validation loss 0.0403\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size = AB_string_3.vocab_size\n",
    "input_size = AB_string_3.vocab_size\n",
    "num_layers = 1\n",
    "m3 = LanguageModel_LSTM(input_size, hidden_size, num_layers, verbose=False)\n",
    "\n",
    "# check code is doing what I expect it to do. \n",
    "# Keep batch_size and block_size small to allow easy visualisation of \n",
    "# what is going on inside the network, to check that it's doing what I \n",
    "# think it's doing.\n",
    "# Uncommented print statements in code\n",
    "batch_size=2\n",
    "block_size =4\n",
    "number_of_evaluation_batches = 10\n",
    "evaluation_interval = 500\n",
    "# high learning rate since simple network\n",
    "learning_rate = 1e-3\n",
    "#logits, loss = m(train_context_batch, train_to_predict_batch)\n",
    "#print(logits.shape) \n",
    "#print(loss)\n",
    "\n",
    "# training loop. \n",
    "optimizer = torch.optim.AdamW(m3.parameters(), lr=learning_rate) \n",
    "for step in range(4000):\n",
    "  # get a new batch\n",
    "  xb, yb = AB_string_3.get_batch(batch_size, block_size, 'train')\n",
    "  logits, loss = m3(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "   #equally uncertain distribution implies 1/vocab_size = 1/65 probability of each \n",
    "   # next character which implies -ln(1/65) = 4.17 would be optimal initial loss\n",
    "  if step % evaluation_interval == 0: \n",
    "    print(f\"current iteration loss, at step {step}: {loss.item()}\")\n",
    "  if step % evaluation_interval == 0:\n",
    "    #logits_val, loss_val = m(xval, yval)\n",
    "    #print(f\"training loss: {loss.item()}, \\\n",
    "    #                                       validation loss {loss_val.item()}\")\n",
    "    losses = estimate_loss(number_of_evaluation_batches, m3, AB_string_3, block_size)\n",
    "    print(f\"step {step}: \\n \\\n",
    "           loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "           \\n training loss: {losses['train']:.4f}, \\\n",
    "          \\n validation loss {losses['validation']:.4f}\\n\\n\")\n",
    "losses = estimate_loss(number_of_evaluation_batches, m3, AB_string_3, block_size)\n",
    "print(f\"final loss: \\n \\\n",
    "        loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "        \\n training loss: {losses['train']:.4f}, \\\n",
    "        \\n validation loss {losses['validation']:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([[0]])\n",
      "tensor([[ 0, 13, 14, 15, 16,  0, 17, 18, 19, 20, 21,  0, 22, 23, 24, 25, 26,  0,\n",
      "          1,  2,  3,  4,  0,  5,  6,  7]])\n",
      " MNOP QRSTU VWXYZ ABCD EFG\n"
     ]
    }
   ],
   "source": [
    "context = ' '\n",
    "context = AB_string_3.encode_new_text_as_tensor(context)\n",
    "print(context)\n",
    "context = torch.unsqueeze(context, dim=0)\n",
    "print(context)\n",
    "m3.set_verbose(False)\n",
    "new_string = m3.generate(context,25)\n",
    "print(new_string)\n",
    "print(AB_string_3.decode(new_string[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeare\n",
      "reading text from file\n",
      "chars, ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "# Next, tiny Shakespeare\n",
    "print(\"shakespeare\")\n",
    "shakespeare = text(\"input.txt\", is_file=True)\n",
    "#shakespeare.from_file(\"input.txt\")\n",
    "print(f\"chars, {shakespeare.chars}\")\n",
    "print(f\"vocab size: {shakespeare.vocab_size}\")\n",
    "shakespeare.encode_text_as_tensor()\n",
    "shakespeare.make_train_val_test(0.8, 0.1, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current iteration loss, at step 0: 4.202329158782959\n",
      "step 0: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 4.1947,           \n",
      " validation loss 4.1932\n",
      "\n",
      "\n",
      "current iteration loss, at step 500: 2.594022512435913\n",
      "step 500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 2.5751,           \n",
      " validation loss 2.5946\n",
      "\n",
      "\n",
      "current iteration loss, at step 1000: 2.3799805641174316\n",
      "step 1000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 2.3986,           \n",
      " validation loss 2.4757\n",
      "\n",
      "\n",
      "current iteration loss, at step 1500: 2.296194553375244\n",
      "step 1500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 2.2671,           \n",
      " validation loss 2.2999\n",
      "\n",
      "\n",
      "current iteration loss, at step 2000: 2.183516263961792\n",
      "step 2000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 2.1854,           \n",
      " validation loss 2.2326\n",
      "\n",
      "\n",
      "current iteration loss, at step 2500: 2.0272293090820312\n",
      "step 2500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 2.1292,           \n",
      " validation loss 2.1277\n",
      "\n",
      "\n",
      "current iteration loss, at step 3000: 1.9452658891677856\n",
      "step 3000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 2.0015,           \n",
      " validation loss 2.0356\n",
      "\n",
      "\n",
      "current iteration loss, at step 3500: 1.8866448402404785\n",
      "step 3500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.9548,           \n",
      " validation loss 2.0756\n",
      "\n",
      "\n",
      "final loss: \n",
      "         loss averaged over 20 batches:        \n",
      " training loss: 1.9725,         \n",
      " validation loss 2.0138\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size = shakespeare.vocab_size*2\n",
    "input_size = shakespeare.vocab_size\n",
    "num_layers = 3\n",
    "shakespeare_predictor = LanguageModel_LSTM(input_size, \\\n",
    "                                hidden_size, num_layers, verbose=False)\n",
    "\n",
    "# check code is doing what I expect it to do. \n",
    "# Keep batch_size and block_size small to allow easy visualisation of \n",
    "# what is going on inside the network, to check that it's doing what I \n",
    "# think it's doing.\n",
    "\n",
    "batch_size=20\n",
    "block_size =100\n",
    "number_of_evaluation_batches = 20\n",
    "evaluation_interval = 500\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# training loop. \n",
    "optimizer =\\\n",
    "      torch.optim.AdamW(shakespeare_predictor.parameters(), lr=learning_rate) \n",
    "for step in range(4000):\n",
    "  # get a new batch\n",
    "  xb, yb = shakespeare.get_batch(batch_size, block_size, 'train')\n",
    "  logits, loss = shakespeare_predictor(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  if step % evaluation_interval == 0: \n",
    "    print(f\"current iteration loss, at step {step}: {loss.item()}\")\n",
    "  if step % evaluation_interval == 0:\n",
    "    losses = estimate_loss(number_of_evaluation_batches, \\\n",
    "                           shakespeare_predictor, shakespeare, block_size)\n",
    "    print(f\"step {step}: \\n \\\n",
    "           loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "           \\n training loss: {losses['train']:.4f}, \\\n",
    "          \\n validation loss {losses['validation']:.4f}\\n\\n\")\n",
    "losses = estimate_loss(number_of_evaluation_batches,\\\n",
    "                        shakespeare_predictor, shakespeare, block_size)\n",
    "print(f\"final loss: \\n \\\n",
    "        loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "        \\n training loss: {losses['train']:.4f}, \\\n",
    "        \\n validation loss {losses['validation']:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current iteration loss, at step 0: 1.9088289737701416\n",
      "step 0: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 2.1291,           \n",
      " validation loss 2.2228\n",
      "\n",
      "\n",
      "current iteration loss, at step 500: 1.8210737705230713\n",
      "step 500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.8910,           \n",
      " validation loss 1.9360\n",
      "\n",
      "\n",
      "current iteration loss, at step 1000: 1.863354206085205\n",
      "step 1000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.8833,           \n",
      " validation loss 1.9177\n",
      "\n",
      "\n",
      "current iteration loss, at step 1500: 1.8386300802230835\n",
      "step 1500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.8516,           \n",
      " validation loss 1.9047\n",
      "\n",
      "\n",
      "current iteration loss, at step 2000: 1.7058151960372925\n",
      "step 2000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.7743,           \n",
      " validation loss 1.7944\n",
      "\n",
      "\n",
      "current iteration loss, at step 2500: 1.808297872543335\n",
      "step 2500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.8004,           \n",
      " validation loss 1.8385\n",
      "\n",
      "\n",
      "current iteration loss, at step 3000: 1.7221351861953735\n",
      "step 3000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.7319,           \n",
      " validation loss 1.8397\n",
      "\n",
      "\n",
      "current iteration loss, at step 3500: 1.7230310440063477\n",
      "step 3500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.6802,           \n",
      " validation loss 1.8144\n",
      "\n",
      "\n",
      "current iteration loss, at step 4000: 1.6663737297058105\n",
      "step 4000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.6969,           \n",
      " validation loss 1.7940\n",
      "\n",
      "\n",
      "current iteration loss, at step 4500: 1.6280858516693115\n",
      "step 4500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.7325,           \n",
      " validation loss 1.7806\n",
      "\n",
      "\n",
      "current iteration loss, at step 5000: 1.6404366493225098\n",
      "step 5000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.5803,           \n",
      " validation loss 1.7273\n",
      "\n",
      "\n",
      "current iteration loss, at step 5500: 1.651208758354187\n",
      "step 5500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.6196,           \n",
      " validation loss 1.7816\n",
      "\n",
      "\n",
      "current iteration loss, at step 6000: 1.666293740272522\n",
      "step 6000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.6421,           \n",
      " validation loss 1.7443\n",
      "\n",
      "\n",
      "current iteration loss, at step 6500: 1.56607186794281\n",
      "step 6500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.6414,           \n",
      " validation loss 1.6616\n",
      "\n",
      "\n",
      "current iteration loss, at step 7000: 1.5376321077346802\n",
      "step 7000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.6424,           \n",
      " validation loss 1.7176\n",
      "\n",
      "\n",
      "current iteration loss, at step 7500: 1.5640485286712646\n",
      "step 7500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.5429,           \n",
      " validation loss 1.7642\n",
      "\n",
      "\n",
      "current iteration loss, at step 8000: 1.5553600788116455\n",
      "step 8000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.5087,           \n",
      " validation loss 1.7233\n",
      "\n",
      "\n",
      "current iteration loss, at step 8500: 1.5670092105865479\n",
      "step 8500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4880,           \n",
      " validation loss 1.8013\n",
      "\n",
      "\n",
      "current iteration loss, at step 9000: 1.5985347032546997\n",
      "step 9000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.5973,           \n",
      " validation loss 1.6639\n",
      "\n",
      "\n",
      "current iteration loss, at step 9500: 1.5576977729797363\n",
      "step 9500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.5631,           \n",
      " validation loss 1.6211\n",
      "\n",
      "\n",
      "final loss: \n",
      "         loss averaged over 20 batches:        \n",
      " training loss: 1.5915,         \n",
      " validation loss 1.6741\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# continue with another 10 000 steps - loss is coming down slowly\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# training loop. \n",
    "optimizer =\\\n",
    "      torch.optim.AdamW(shakespeare_predictor.parameters(), lr=learning_rate) \n",
    "for step in range(10000):\n",
    "  # get a new batch\n",
    "  xb, yb = shakespeare.get_batch(batch_size, block_size, 'train')\n",
    "  logits, loss = shakespeare_predictor(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  if step % evaluation_interval == 0: \n",
    "    print(f\"current iteration loss, at step {step}: {loss.item()}\")\n",
    "  if step % evaluation_interval == 0:\n",
    "    losses = estimate_loss(number_of_evaluation_batches, \\\n",
    "                           shakespeare_predictor, shakespeare, block_size)\n",
    "    print(f\"step {step}: \\n \\\n",
    "           loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "           \\n training loss: {losses['train']:.4f}, \\\n",
    "          \\n validation loss {losses['validation']:.4f}\\n\\n\")\n",
    "losses = estimate_loss(number_of_evaluation_batches,\\\n",
    "                        shakespeare_predictor, shakespeare, block_size)\n",
    "print(f\"final loss: \\n \\\n",
    "        loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "        \\n training loss: {losses['train']:.4f}, \\\n",
    "        \\n validation loss {losses['validation']:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([[1]])\n",
      "tensor([[ 1, 51, 63,  1, 54, 56, 43, 43, 58,  8,  1, 13, 46, 61, 53, 56, 42,  2,\n",
      "          1, 44, 56, 59, 50, 57, 43,  8,  0,  0, 15, 27, 30, 21, 27, 24, 13, 26,\n",
      "         33, 31, 10,  0, 14, 43, 41, 56, 47, 60, 43,  1, 39, 52, 42,  1, 51, 39,\n",
      "         52, 63,  1, 61, 47, 58, 46,  1, 40, 43, 53, 52,  6,  0, 26, 53,  6,  1,\n",
      "         47, 58,  1, 57, 46, 39, 50, 50,  1, 47, 58, 51, 43, 52, 43, 57, 57,  8,\n",
      "          0,  0, 29, 33, 17, 17, 26,  1, 17, 24, 21]])\n",
      " my preet. Ahword! frulse.\n",
      "\n",
      "CORIOLANUS:\n",
      "Becrive and many with beon,\n",
      "No, it shall itmeness.\n",
      "\n",
      "QUEEN ELI\n"
     ]
    }
   ],
   "source": [
    "# current model seems to be decreasing in loss, albeit very slowly\n",
    "# try some generative function and then train some more.\n",
    "context = ' '\n",
    "context = shakespeare.encode_new_text_as_tensor(context)\n",
    "print(context)\n",
    "context = torch.unsqueeze(context, dim=0)\n",
    "print(context)\n",
    "shakespeare_predictor.set_verbose(False)\n",
    "new_string = shakespeare_predictor.generate(context,100)\n",
    "print(new_string)\n",
    "print(shakespeare.decode(new_string[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current iteration loss, at step 0: 1.579905390739441\n",
      "step 0: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.7015,           \n",
      " validation loss 1.8380\n",
      "\n",
      "\n",
      "current iteration loss, at step 500: 1.4419159889221191\n",
      "step 500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4974,           \n",
      " validation loss 1.6658\n",
      "\n",
      "\n",
      "current iteration loss, at step 1000: 1.5601158142089844\n",
      "step 1000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4682,           \n",
      " validation loss 1.5732\n",
      "\n",
      "\n",
      "current iteration loss, at step 1500: 1.5540715456008911\n",
      "step 1500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4807,           \n",
      " validation loss 1.6578\n",
      "\n",
      "\n",
      "current iteration loss, at step 2000: 1.5376372337341309\n",
      "step 2000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.5385,           \n",
      " validation loss 1.6355\n",
      "\n",
      "\n",
      "current iteration loss, at step 2500: 1.5004305839538574\n",
      "step 2500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4606,           \n",
      " validation loss 1.5659\n",
      "\n",
      "\n",
      "current iteration loss, at step 3000: 1.6521944999694824\n",
      "step 3000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4671,           \n",
      " validation loss 1.6117\n",
      "\n",
      "\n",
      "current iteration loss, at step 3500: 1.452095866203308\n",
      "step 3500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4899,           \n",
      " validation loss 1.5665\n",
      "\n",
      "\n",
      "current iteration loss, at step 4000: 1.4422298669815063\n",
      "step 4000: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.5110,           \n",
      " validation loss 1.6349\n",
      "\n",
      "\n",
      "current iteration loss, at step 4500: 1.4319647550582886\n",
      "step 4500: \n",
      "            loss averaged over 20 batches:           \n",
      " training loss: 1.4399,           \n",
      " validation loss 1.6205\n",
      "\n",
      "\n",
      "final loss: \n",
      "         loss averaged over 20 batches:        \n",
      " training loss: 1.4537,         \n",
      " validation loss 1.6597\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# continue with another 10 000 steps - loss is coming down slowly\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# training loop. \n",
    "optimizer =\\\n",
    "      torch.optim.AdamW(shakespeare_predictor.parameters(), lr=learning_rate) \n",
    "for step in range(5000):\n",
    "  # get a new batch\n",
    "  xb, yb = shakespeare.get_batch(batch_size, block_size, 'train')\n",
    "  logits, loss = shakespeare_predictor(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  if step % evaluation_interval == 0: \n",
    "    print(f\"current iteration loss, at step {step}: {loss.item()}\")\n",
    "  if step % evaluation_interval == 0:\n",
    "    losses = estimate_loss(number_of_evaluation_batches, \\\n",
    "                           shakespeare_predictor, shakespeare, block_size)\n",
    "    print(f\"step {step}: \\n \\\n",
    "           loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "           \\n training loss: {losses['train']:.4f}, \\\n",
    "          \\n validation loss {losses['validation']:.4f}\\n\\n\")\n",
    "losses = estimate_loss(number_of_evaluation_batches,\\\n",
    "                        shakespeare_predictor, shakespeare, block_size)\n",
    "print(f\"final loss: \\n \\\n",
    "        loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "        \\n training loss: {losses['train']:.4f}, \\\n",
    "        \\n validation loss {losses['validation']:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([[1]])\n",
      "tensor([[ 1, 46, 43, 50, 50,  6,  1, 50, 43, 61, 57,  6,  1, 39, 57, 41, 43, 56,\n",
      "         58, 50, 43, 11,  1, 21,  1, 39, 51,  1, 58, 46, 47, 57,  1, 42, 43, 39,\n",
      "         44, 10,  0, 37, 53, 59, 56,  1, 53, 58, 46, 43, 56,  1, 61, 46, 47, 50,\n",
      "         43,  1, 53, 52, 43,  1, 58, 46, 39, 58,  1, 56, 59, 58, 41, 46, 43, 56,\n",
      "         52, 58,  1, 53, 52,  1, 51, 43, 10,  0, 21,  1, 57, 46, 39, 50, 50,  1,\n",
      "         46, 39, 60, 43,  1, 58, 39, 49, 43,  1, 53, 44,  1, 30, 47, 41, 46, 51,\n",
      "         53, 52, 42,  1, 47, 57,  6,  0, 33, 52, 58, 43, 47, 45, 52, 39, 50, 57,\n",
      "          1, 47, 52,  1, 51, 63, 57, 43, 50, 44,  6,  1, 58, 46, 39, 58,  1, 58,\n",
      "         47, 50, 50,  1, 56, 39, 52]])\n",
      " hell, lews, ascertle; I am this deaf:\n",
      "Your other while one that rutchernt on me:\n",
      "I shall have take of Richmond is,\n",
      "Unteignals in myself, that till ran\n"
     ]
    }
   ],
   "source": [
    "# current model seems to be still decreasing in loss, albeit very slowly\n",
    "# try some generative function and then train some more.\n",
    "context = ' '\n",
    "context = shakespeare.encode_new_text_as_tensor(context)\n",
    "print(context)\n",
    "context = torch.unsqueeze(context, dim=0)\n",
    "print(context)\n",
    "shakespeare_predictor.set_verbose(False)\n",
    "new_string = shakespeare_predictor.generate(context,150)\n",
    "print(new_string)\n",
    "print(shakespeare.decode(new_string[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
