{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Herein are cells that \n",
    "# 1. Create a text class that has methods to determine the vocabulary\n",
    "# within the text, encode/decode the text, split the text content into\n",
    "# validation, training and test sets.\n",
    "# \n",
    "# 2. LSTM language model for training and generating text.\n",
    "#\n",
    "# 3. Reading in of a simple test case\n",
    "#  Training and validation loop for simple test case.\n",
    "#\n",
    "# 4. Tiny Shakespeare training and generation. \n",
    "# \n",
    "\n",
    "# Refactoring - there is a logic in moving the vocab/encoding/decoding \n",
    "# methods from the text class into the language model, since the language\n",
    "# model has to deal with with an appropriately encoded/decoded \n",
    "# set of data - so saving the weights of the language model is useless\n",
    "# without knowing the encodings! On the otherhand ... that's to \n",
    "# put two different types of things in the same object ... so leave as\n",
    "# is for the moment. \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class text:\n",
    "    def __init__(self, text_name, is_file):\n",
    "        \n",
    "        # Retrieve text\n",
    "        self.text = ''\n",
    "        self.encoded = ''\n",
    "        if is_file == True:\n",
    "            print(\"reading text from file\")\n",
    "            self.from_file(text_name)\n",
    "        else:\n",
    "            self.from_string(text_name)\n",
    "        \n",
    "        # Calculate vocab size, i.e. the number of characters; \n",
    "        # first get sorted list of unique characters\n",
    "        self.chars = sorted(list(set(self.text)))\n",
    "        #self.vocab_size = self.vocab_size(chars)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.str_to_int = {}\n",
    "        self.int_to_str = {}\n",
    "        self.str_to_int = self.make_str_to_int_table(self.chars)\n",
    "        self.int_to_str = self.make_int_to_str_table(self.chars)\n",
    "    \n",
    "    def from_file(self, filename):\n",
    "        'Read text from file and calculate vocab size'\n",
    "        self.text = open(filename,'r',encoding='utf-8').read()\n",
    "    \n",
    "    def from_string(self, string):\n",
    "        'Read text from string and calculate vocab size'\n",
    "        self.text = string\n",
    "    \n",
    "    #def calc_vocab_size(self, chars):\n",
    "    #    'Calculate vocab size'\n",
    "    #    self.vocab_size = len(chars)\n",
    "        \n",
    "    def make_str_to_int_table(self, chars):\n",
    "        '''Populate dictionary of character to integer mapping'''\n",
    "        return  {character: int for int, character in enumerate(chars)}\n",
    "    \n",
    "    def make_int_to_str_table(self, chars):\n",
    "        '''Populate dictionary of character to integer mapping'''\n",
    "        return  {int: character for int, character in enumerate(chars)}\n",
    "    \n",
    "    def encode_text_as_tensor(self):\n",
    "        '''encode the training text as a list of integers \n",
    "        and then convert to tensor with which to replace self.text'''\n",
    "        encode = lambda char:[self.str_to_int[char] for char in self.text]\n",
    "        self.text = torch.tensor(encode(self.text), dtype = torch.long)\n",
    "    \n",
    "    def encode_new_text_as_tensor(self, to_encode):\n",
    "        '''encode a new text as a list of integers, according to the \n",
    "        encoding derived from the training text. Return a tensor'''\n",
    "        encode = lambda char:[self.str_to_int[char] for char in to_encode]\n",
    "        return torch.tensor(encode(to_encode), dtype = torch.long)\n",
    "\n",
    "    def decode(self, to_decode):\n",
    "        '''decode from a list of integers to a string, using the\n",
    "        encoding vocabulary attached to the object: self.int_to_str '''  \n",
    "        decode = lambda l: ''.join([self.int_to_str[i] for i in l])\n",
    "        return decode(to_decode)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.vocab_size}\"\n",
    "    \n",
    "    def make_train_val_test(self, fraction_train, fraction_val, fraction_test):\n",
    "        '''simple train /validation sets.  no randomisation \n",
    "         of selections, so assuming  no bias in the distribution within the data file'''\n",
    "        if fraction_train + fraction_test + fraction_val != 1:\n",
    "            print(\"Warning, fractions of train, test and validation do \\\n",
    "                  not add to one.\")\n",
    "        n = int(fraction_train*len(self.text))\n",
    "        nv = int(fraction_val*len(self.text))\n",
    "        nt = int(fraction_test*len(self.text))\n",
    "        self.train_data = self.text[:n]\n",
    "        self.val_data = self.text[n:n+nv]\n",
    "        self.test_data = self.text[n+nv:n+nv+nt]\n",
    "    \n",
    "    def get_batch(self, batch_size, block_size, train_test_validation):\n",
    "        \"\"\"Randomly pick data from the training data/test data/validation\n",
    "        and return as a batch stacked in a torch tensor.\"\"\"\n",
    "        if train_test_validation == \"train\":\n",
    "            data = self.train_data\n",
    "        elif train_test_validation == \"test\":\n",
    "            data = self.test_data\n",
    "        elif train_test_validation == \"validation\":\n",
    "            data = self.val_data\n",
    "        else:\n",
    "            raise \\\n",
    "            ValueError(\"Specify data set as 'train', 'test', or 'validation'.\")\n",
    "\n",
    "        if len(data) < block_size:\n",
    "            raise ValueError(\"Data is smaller than the specified block size.\")\n",
    "\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        train_context_batch = torch.stack([data[i:i + block_size] for i in ix])\n",
    "        train_to_predict_batch =\\\n",
    "              torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "        return train_context_batch, train_to_predict_batch\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(number_eval_batches, \n",
    "                  language_model, training_text_object, block_size):\n",
    "  '''Take number_eval_batches from training and validation sets and \n",
    "  calculate an average loss for each. Return a dictionary with the\n",
    "  two losses, with keys train and validation '''\n",
    "  out = {}\n",
    "  language_model.eval()\n",
    "  for split in ['train', 'validation']:\n",
    "    losses = torch.zeros(number_eval_batches)\n",
    "    for k in range(number_eval_batches):\n",
    "      X, Y = training_text_object.get_batch(1, block_size, split)\n",
    "      logits, loss = language_model(X, Y)\n",
    "      losses[k] = loss.item()\n",
    "    out[split] = losses.mean()\n",
    "  language_model.train()\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM  \n",
    "#\n",
    "class LanguageModel_LSTM(nn.Module):\n",
    "  '''logits produced from an LSTM over the context (length block_size) \n",
    "   of the previous characters, \n",
    "    from which we are trying to predict the current character; \n",
    "  generate() uses logits as a multivariate distribution from which \n",
    "    to predict next character. \n",
    "  \n",
    "   Training learns the values in the embedding vector, the weights of \n",
    "    RNN and the weights of the fully connected layer prior to the\n",
    "     softmax '''\n",
    "\n",
    "  def __init__(self, vocab_size, hidden_size, num_layers, verbose=False):\n",
    "    super().__init__()\n",
    "    self.verbose=verbose\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    # make all embeddings small so the logits produced give a\n",
    "    # probability distribution is equal in all direction.\n",
    "    # maximum initial entropy should give the most likely low cross entropy, \n",
    "    # since initial guess is not likely to be better than equal uncertainty.\n",
    "    with torch.no_grad():\n",
    "      self.token_embedding_table.weight.data \\\n",
    "      = self.token_embedding_table.weight.data * 0.01\n",
    "    self.input_size = vocab_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers,\\\n",
    "                       batch_first=True) # (h0, c0) default to zero.\n",
    "    self.fully_connected = nn.Sequential(\n",
    "                       nn.Linear(hidden_size, hidden_size),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(hidden_size, vocab_size)\n",
    "                       )\n",
    "\n",
    "    \n",
    "  def set_verbose(self, verbose):\n",
    "    self.verbose = verbose\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # idx - (index of) the x values, i.e. the context vector\n",
    "    # idx and targets are both (B, T) tensor of integers (see comment below)\n",
    "    #h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "    # default h0 for RNN model  is to set everything to zeros\n",
    "    logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "    # nn.RNN produces batch_size, sequ, hidden_size,\n",
    "    out, _ = self.lstm(logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #logits = self.fully_connected(logits)\n",
    "    logits = self.fully_connected(out)\n",
    "    # B T C is batch by time by channel\n",
    "    # batch  is number of the batch\n",
    "    # time is block size\n",
    "    # channel is the vocab size \n",
    "    if targets == None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape # need to reshape to be correct shape for cross_entropy\n",
    "      # use view in pytorch that changes the view of the data passed but not the\n",
    "      # underlying data. Flatten the first two dimensions of the logits, \n",
    "      # to create\n",
    "      # a batch of size B*T with the channel data,\n",
    "      # i.e. vocab or probability of each character/class as the second\n",
    "      # dimension, which is what F.cross_entropy expects.\n",
    "      # Similarly, targets reduced to 1 dimension of batch data (B*T) \n",
    "      # with class lable in each dimension. \n",
    "      if self.verbose: print(f\"Targets: \\n {targets} \\n; logits: \\n{logits} \")\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      if self.verbose: print(f\"Transformed view: Targets: \\n {targets} \\n; logits:\\n{logits}\\n\")\n",
    "      # cross_entropy includes a softmax transformation. \n",
    "      if self.verbose:\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        idx_next = torch.multinomial(probs, num_samples = 1) \n",
    "        print(f\"Generative outcomes during training:\\n {idx_next}\")\n",
    "\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    '''Predict the next character/token from the learnt distribution,adding\n",
    "    it to the current context, idx, till max_new_tokens have been added.'''\n",
    "    for _ in range(max_new_tokens):\n",
    "      #get the predictions\n",
    "      logits, _ = self(idx) # calls forwards() for this class\n",
    "      \n",
    "      # Following line: \n",
    "      # h_out from the last character of each sequence in the batch.\n",
    "      logits = logits[:, -1, :] \n",
    "      # logits now has dimension B, C, since only one token.\n",
    "      \n",
    "      # Generate probability distribution\n",
    "      probs = F.softmax(logits, dim=-1) # B, C\n",
    "      if self.verbose:\n",
    "          print(f'logits {logits}, {logits.shape} \\n probs {probs}, {probs.shape}')\n",
    "      \n",
    "      # Sample  vocabulary according to the probability distribution, probs.\n",
    "      idx_next = torch.multinomial(probs, num_samples = 1) # B by 1 size\n",
    "      # Append sample to sequence\n",
    "      idx = torch.cat((idx, idx_next), dim = 1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  ..., 25, 26,  0])\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Train Simple RNN language model on \"ABCD EFGH IJKL MNOP QRSTU VWXYZ \" string.\n",
    "# Define string\n",
    "# Read string into object\n",
    "# Encode string and place in a tensor\n",
    "string = \"ABCD EFGH IJKL MNOP QRSTU VWXYZ \"\n",
    "string = string * 100\n",
    "AB_string_3 = text(string, is_file=False)\n",
    "AB_string_3.encode_text_as_tensor()\n",
    "AB_string_3.make_train_val_test(0.8, 0.1, 0.1)\n",
    "print(AB_string_3.train_data)\n",
    "print(AB_string_3.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current iteration loss, at step 0: 3.353686571121216\n",
      "step 0: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 3.3386,           \n",
      " validation loss 3.3218\n",
      "\n",
      "\n",
      "current iteration loss, at step 500: 1.8055510520935059\n",
      "step 500: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 1.3512,           \n",
      " validation loss 1.1075\n",
      "\n",
      "\n",
      "current iteration loss, at step 1000: 0.29412171244621277\n",
      "step 1000: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.3203,           \n",
      " validation loss 0.3035\n",
      "\n",
      "\n",
      "current iteration loss, at step 1500: 0.12038694322109222\n",
      "step 1500: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.1839,           \n",
      " validation loss 0.2092\n",
      "\n",
      "\n",
      "current iteration loss, at step 2000: 0.04319122061133385\n",
      "step 2000: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.2336,           \n",
      " validation loss 0.1353\n",
      "\n",
      "\n",
      "current iteration loss, at step 2500: 0.006636984646320343\n",
      "step 2500: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.1043,           \n",
      " validation loss 0.0468\n",
      "\n",
      "\n",
      "current iteration loss, at step 3000: 0.006047433707863092\n",
      "step 3000: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.1018,           \n",
      " validation loss 0.0072\n",
      "\n",
      "\n",
      "current iteration loss, at step 3500: 0.005555641837418079\n",
      "step 3500: \n",
      "            loss averaged over 10 batches:           \n",
      " training loss: 0.1859,           \n",
      " validation loss 0.1572\n",
      "\n",
      "\n",
      "final loss: \n",
      "         loss averaged over 10 batches:        \n",
      " training loss: 0.1595,         \n",
      " validation loss 0.1294\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size = AB_string_3.vocab_size\n",
    "input_size = AB_string_3.vocab_size\n",
    "num_layers = 1\n",
    "m3 = LanguageModel_LSTM(input_size, hidden_size, num_layers, verbose=False)\n",
    "\n",
    "# check code is doing what I expect it to do. \n",
    "# Keep batch_size and block_size small to allow easy visualisation of \n",
    "# what is going on inside the network, to check that it's doing what I \n",
    "# think it's doing.\n",
    "# Uncommented print statements in code\n",
    "batch_size=2\n",
    "block_size =4\n",
    "number_of_evaluation_batches = 10\n",
    "evaluation_interval = 500\n",
    "# high learning rate since simple network\n",
    "learning_rate = 1e-3\n",
    "#logits, loss = m(train_context_batch, train_to_predict_batch)\n",
    "#print(logits.shape) \n",
    "#print(loss)\n",
    "\n",
    "# training loop. \n",
    "optimizer = torch.optim.AdamW(m3.parameters(), lr=learning_rate) \n",
    "for step in range(4000):\n",
    "  # get a new batch\n",
    "  xb, yb = AB_string_3.get_batch(batch_size, block_size, 'train')\n",
    "  logits, loss = m3(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "   #equally uncertain distribution implies 1/vocab_size = 1/65 probability of each \n",
    "   # next character which implies -ln(1/65) = 4.17 would be optimal initial loss\n",
    "  if step % evaluation_interval == 0: \n",
    "    print(f\"current iteration loss, at step {step}: {loss.item()}\")\n",
    "  if step % evaluation_interval == 0:\n",
    "    #logits_val, loss_val = m(xval, yval)\n",
    "    #print(f\"training loss: {loss.item()}, \\\n",
    "    #                                       validation loss {loss_val.item()}\")\n",
    "    losses = estimate_loss(number_of_evaluation_batches, m3, AB_string_3, block_size)\n",
    "    print(f\"step {step}: \\n \\\n",
    "           loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "           \\n training loss: {losses['train']:.4f}, \\\n",
    "          \\n validation loss {losses['validation']:.4f}\\n\\n\")\n",
    "losses = estimate_loss(number_of_evaluation_batches, m3, AB_string_3, block_size)\n",
    "print(f\"final loss: \\n \\\n",
    "        loss averaged over {number_of_evaluation_batches} batches:\\\n",
    "        \\n training loss: {losses['train']:.4f}, \\\n",
    "        \\n validation loss {losses['validation']:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([[0]])\n",
      "tensor([[ 0,  5,  6,  7,  8,  0,  9, 10, 11, 12,  0, 13, 14, 15, 16,  0, 17, 18,\n",
      "         19, 20, 21,  0, 22, 23, 24, 25]])\n",
      " EFGH IJKL MNOP QRSTU VWXY\n"
     ]
    }
   ],
   "source": [
    "context = ' '\n",
    "context = AB_string_3.encode_new_text_as_tensor(context)\n",
    "print(context)\n",
    "context = torch.unsqueeze(context, dim=0)\n",
    "print(context)\n",
    "m3.set_verbose(False)\n",
    "new_string = m3.generate(context,25)\n",
    "print(new_string)\n",
    "print(AB_string_3.decode(new_string[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, tiny Shakespeare\n",
    "print(\"shakespeare\")\n",
    "shakespeare = text(\"input.txt\", is_file=True)\n",
    "#shakespeare.from_file(\"input.txt\")\n",
    "print(f\"chars, {shakespeare.chars}\")\n",
    "print(f\"vocab size: {shakespeare.vocab_size}\")\n",
    "shakespeare.encode_text_as_tensor()\n",
    "shakespeare.make_train_val_test(0.8, 0.1, 0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
